{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第６回：Chainer入門\n",
    "\n",
    "## 到達目標\n",
    "* Chainerを使って全結合のニューラルネットを記述し、実行することができる\n",
    "* ニューラルネットとその他の手法との関係性とその特性の差を理解する\n",
    "* ニューラルネットの基本的な学習テクニックを学ぶ\n",
    "\n",
    "## キーワード\n",
    "* Chainer\n",
    "* Define-by-Run\n",
    "* Link, Chain, Optimizer\n",
    "\n",
    "## タイムスケジュール\n",
    "### 前回の復習 (5分)\n",
    "### 講義・基礎演習 (85分)\n",
    "#### Part 1. Chainer入門 (50分)\n",
    "##### 講義 (10分)\n",
    "* Chainerとは\n",
    "* \"Define-and-Run\"と\"Define-by-Run\"\n",
    "* Chainerの特長と他フレームワークとの比較\n",
    "\n",
    "##### 基礎演習 (40分)\n",
    "* 計算グラフの記述\n",
    "* Link\n",
    "* Chain\n",
    "* L.Linear, F.relu, F.softmax\\_cross\\_entropy\n",
    "* 多層パーセプトロンのChainerによる記述\n",
    "* Optimizer\n",
    "* Variable\n",
    "* Trainerを用いないニューラルネットの学習\n",
    "* Trainer/Updater\n",
    "* datasets/iterators\n",
    "* Extension(Evaluator, LogReport, PrintReport, ProgressBar, snapshot)\n",
    "* Trainerを用いたニューラルネットの学習\n",
    "* GPU対応コードの実装\n",
    "* モデルの保存と読み込み (Serializer)\n",
    "* 課題:Chainerを用いたロジスティック回帰の実装\n",
    "\n",
    "#### Part 2. ニューラルネットの学習テクニック (15分)\n",
    "* NNの最適化手法(Momentum法(MomentumSGD)/AdaGrad/Adam)\n",
    "* NNの正則化手法(Dropout)\n",
    "* 勾配消失問題とHeの初期化\n",
    "* Batch Normalization\n",
    "\n",
    "#### Part 3. 他手法との比較 (20分)\n",
    "* 汎化誤差・交差検証 (復習)\n",
    "* 機械学習モデルの性能を決める要素 (特徴選択・前処理・線形性)\n",
    "* SVMとNNの比較\n",
    "* モデル選択\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainerとは？\n",
    "\n",
    "## 日本製のディープラーニングフレームワーク\n",
    "\n",
    "<img src=\"./images/01.png\" width=\"400\" />\n",
    "\n",
    "**Chainer**は日本の企業である**Preferred Networks**社が開発をすすめるディープラーニング（ニューラルネットワーク）に特化したPythonで使用できるフレームワークです。\n",
    "他にも、Googleが提供するTensorFlowやそのラッパーのKerasもあり、個人的には日本ではこのどちらかを使っている人が多いかなと感じます。\n",
    "\n",
    "**Chainerはもともと習得が簡単なインターフェースで作られている面**と、他のフレームワークに比べて、ディープラーニングの開発を論文レベルだったりの**カスタマイズをする際に非常に柔軟に対応できる**といった点が魅力に感じています。\n",
    "\n",
    "## 特徴は「Define by Run」\n",
    "**Define by Run**と呼ばれる仕組みがGoogleのTensorFlowをはじめとした他のフレームワークとの大きな違いであり、初心者にとっては、**学習途中に数値やサイズの確認が出来るといったデバックの容易さ**がメリットです（Chainerの開発者から直接聞きました）。  \n",
    "たしかに、**学習途中にどのような挙動をしているか、どこでエラーが起きているか**は開発者にとっては非常に大事なため、この構造を採用しているのは、大きなメリットだと感じます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chainerでわからないことがあれば、開発者にSlackで聞こう！\n",
    "\n",
    "ChainerではPFNの開発者にダイレクトに質問できるSlackがあり、セミナー参加者は正体大歓迎とPFNの方も言ってくださっているので、ぜひ[こちら](https://docs.google.com/forms/d/e/1FAIpQLSfqL9XjnqZUIwLOz4K9Oxm8-Ce246IRP51-vZa7HOrofJT9rA/viewform)からメールアドレスを登録して、Slackに招待してもらいましょう。  \n",
    "[**▶ Chainer Slack 受付フォーム**](https://docs.google.com/forms/d/e/1FAIpQLSfqL9XjnqZUIwLOz4K9Oxm8-Ce246IRP51-vZa7HOrofJT9rA/viewform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算グラフの記述\n",
    "まずは、３ノードの入力層と２ノードの出力層の部分を表現していきましょう。\n",
    "\n",
    "<img src=\"./images/02.png\" width=\"400\" />\n",
    "\n",
    "ノードの結合を表す時は、chainer.linksを使用します。\n",
    "そして、このchainer.linksをLとして宣言するので、覚えておきましょう（公式リファレンス推奨の方法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 -> 2 のリンクをl1として、以下のように宣言します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = L.Linear(3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これだけで完了です。\n",
    "L.Linearの意味は、みなさんが勉強されてた重回帰分析の線形結合という意味です。\n",
    "\n",
    "そして、宣言したリンクの重み（W）とバイアス（b）はランダムに初期化されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable W([[-0.36897632,  0.37382668, -0.23298986],\n",
       "            [-0.46077964, -0.15170476,  0.43850562]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable b([0., 0.])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このランダムに初期化されたパラメータを最適化の初期値に使用します。\n",
    "\n",
    "Chainerの中で必要な基礎はこのリンクの書き方を抑えておけば、一旦OKです。\n",
    "他にも色々な機能がありますが、これは次の実践的な非線形回帰を試しながら、見ていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainerでワインのクラス分類を行おう\n",
    "実際の問題を想定しながら、Chainerの使い方を見ていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを定義しよう\n",
    "\n",
    "<img src=\"./images/03.png\" width=\"400\" />\n",
    "\n",
    "Chainerでは、まずモデルの定義を行います。\n",
    "今回は下記のような、４層のニューラルネットワーク（NN）の作り方を学びましょう。\n",
    "\n",
    "まず、Chainerのモデルの中で使用するchainer.functionsを読み込んでおきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chainer.fuinctionsでは、F.reluとして**Relu関数**や、F.mean_squared_errorとして**平均二乗誤差（Mean Squared Errors）**、F.softmax_cross_entropyとして**ソフトマックスクロスエントロピー**の計算など、ニューラルネットワークでよく使用する関数が定義されています。\n",
    "\n",
    "そして、今回の４層のモデルをChainerでは以下のように書きます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP: Multi-layer Perceptron\n",
    "class MLP(chainer.Chain):\n",
    "    \n",
    "    def __init__(self, n_units1, n_units2, n_output):\n",
    "        super().__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, n_units1)\n",
    "            self.l2 = L.Linear(None, n_units2)\n",
    "            self.l3 = L.Linear(None, n_output)\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return self.l3(h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "少し多いため戸惑いますが、基本的には雛形に分類の問題はカスタマイズするだけで実装できるため、カスタマイズするポイントを覚えておきましょう。\n",
    "\n",
    "- `__init__`：モデルの構造を宣言\n",
    "- `__call__`：評価関数を宣言\n",
    "\n",
    "他にも書き方（この自由度の高さがChainerの魅力の１つ）がありますが、簡単なモデルの場合はこのような構造で書いておくことをおすすめします。\n",
    "\n",
    "`__init__`内にモデルの構造を書き、`__call__`内でそれをつなぎ合わせるイメージです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chainerでは定義するモデルとClassifierを繋げて使う\n",
    "これは、Chainer流の書き方に合わせるためですが、基本的には、Chainerの分類を書く際には、以下の２つのモデルに分けることが多いです。\n",
    "\n",
    "- 順伝播で予測値を計算（推論）を行うモデル　←　今回のMLP\n",
    "- 予測値から評価関数を計算するモデル　←　この後のL.Classifier\n",
    "\n",
    "この書き方は絶対というわけではありませんが、この書き方を覚えておくと、リファレンスを見た際の対応関係がわかりやすいため、おすすめです。\n",
    "\n",
    "評価関数を計算する部分である L.Classifier は既にChainer側で準備されており、その中でSoftmax関数に変換して計算を行ってくれます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル宣言の際の注意\n",
    "\n",
    "また、モデルの宣言の前に気をつけないといけないことがあります。\n",
    "\n",
    "Chainerのchainer.linksで宣言したL.Lienarの中では宣言したタイミングで、重みWやバイアスbがランダムに初期化されていました。\n",
    "そのため、実行毎に結果が異なってしまい、「昨日うまくいったのに、全く同じプログラムでも今日はうまくいかない」といったことがあり得ます。\n",
    "\n",
    "この対策として、**乱数のシード**を固定することがあります。\n",
    "乱数のシードを固定しておくと、毎回同じ結果が得られ、**再現性の確保**ができます。\n",
    "\n",
    "**これはデータ解析の際には非常に重要なため、絶対に忘れないようにしましょう。**\n",
    "\n",
    "Chainerが使用する乱数のシードはPython標準のrandomではなく、Numpyの中で使用されているnumpy.randomであるため、お気をつけください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つぎに、Numpyの乱数のシードを固定しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シードの固定\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そして、モデルを実際に使っていく際には、下記のように宣言をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNモデルに必要なパラメータの設定\n",
    "n_units1 = 10\n",
    "n_units2 = 10\n",
    "n_output = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNモデルの宣言\n",
    "mlp = MLP(n_units1, n_units2, n_output)\n",
    "\n",
    "# 分類用にラップ\n",
    "model = L.Classifier(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NNモデルの中で、ノードの数を直に書いて宣言しても良いのですが、こちらのように、ノードの数を**引数**としておくことで、柔軟に変更できるため、こちらの書き方がおすすめです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizerを定義しよう\n",
    "\n",
    "Chainerではoptimizerと呼ばれる最適化を担当する部分のモジュールがあります。\n",
    "最適化とは**最急降下法**、**確率的勾配法（SGD)**などのアルゴリズムのことです。\n",
    "\n",
    "optimizerはchainer.optimizersにあるため、こちらを読み込みましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化のアルゴリズムには SGD を使用\n",
    "optimizer = optimizers.SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定義したoptimizerと作成したmodelを紐付けるために、optimizer.setup(model)が必要となるため、忘れないようにしましょう。\n",
    "\n",
    "一番シンプルな場合は、このモデルの宣言とoptimizerの宣言で完了です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.setup(model)  # modelと紐付ける"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データを準備しよう\n",
    "\n",
    "<img src=\"./images/04.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSVファイルからの読み込み\n",
    "\n",
    "**CSVファイル**とは聞き慣れている方も多いかと思いますが、データが**カンマ区切り**で表現されているデータのことです。\n",
    "\n",
    "**wine_class.csv**のファイルを準備しているので、こちらを読み込んでみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandasで読み込み\n",
    "CSVファイル含め、データの読み込みには**Pandas**と呼ばれる外部モジュールを使用すると便利です。\n",
    "整理をしておくと、データ解析の基礎となる３つのツールの位置づけはこちらです。\n",
    "\n",
    "- Numpy：数値データの取り扱い（線形代数含め）\n",
    "- Pandas：データベースの操作（小規模のCSVファイル含め）\n",
    "- Matplotlib：プロット\n",
    "\n",
    "特に、PandasはJupyter Notebookとの相性も良く、綺麗に表を表示することができるため、おすすめです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、Pandasを読み込みましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ローカルのファイルから読み込む\n",
    "df = pd.read_csv('wine_class.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**df** は **data frame** の略であり、pandasの例でよく出てくる名前ですので、覚えておきましょう。\n",
    "\n",
    "Pandasで読み込んだデータはdfで内容を確認でき、df.head()と実行すると、先頭の5つが表示され、長すぎないのでおすすめです。\n",
    "なお、df.head(10)とすると、先頭の10個が表示されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>14.20</td>\n",
       "      <td>2.45</td>\n",
       "      <td>15.2</td>\n",
       "      <td>112</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>14.39</td>\n",
       "      <td>2.45</td>\n",
       "      <td>14.6</td>\n",
       "      <td>96</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.30</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>14.06</td>\n",
       "      <td>2.61</td>\n",
       "      <td>17.6</td>\n",
       "      <td>121</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.31</td>\n",
       "      <td>5.05</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>14.83</td>\n",
       "      <td>2.17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.29</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>13.86</td>\n",
       "      <td>2.27</td>\n",
       "      <td>16.0</td>\n",
       "      <td>98</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>7.22</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Alcohol   Ash  Alcalinity of ash  Magnesium  Total phenols  \\\n",
       "0      1    14.23  2.43               15.6        127           2.80   \n",
       "1      1    13.20  2.14               11.2        100           2.65   \n",
       "2      1    13.16  2.67               18.6        101           2.80   \n",
       "3      1    14.37  2.50               16.8        113           3.85   \n",
       "4      1    13.24  2.87               21.0        118           2.80   \n",
       "5      1    14.20  2.45               15.2        112           3.27   \n",
       "6      1    14.39  2.45               14.6         96           2.50   \n",
       "7      1    14.06  2.61               17.6        121           2.60   \n",
       "8      1    14.83  2.17               14.0         97           2.80   \n",
       "9      1    13.86  2.27               16.0         98           2.98   \n",
       "\n",
       "   Flavanoids  Nonflavanoid phenols  Color intensity   Hue  Proline  \n",
       "0        3.06                  0.28             5.64  1.04     1065  \n",
       "1        2.76                  0.26             4.38  1.05     1050  \n",
       "2        3.24                  0.30             5.68  1.03     1185  \n",
       "3        3.49                  0.24             7.80  0.86     1480  \n",
       "4        2.69                  0.39             4.32  1.04      735  \n",
       "5        3.39                  0.34             6.75  1.05     1450  \n",
       "6        2.52                  0.30             5.25  1.02     1290  \n",
       "7        2.51                  0.31             5.05  1.06     1295  \n",
       "8        2.98                  0.29             5.20  1.08     1045  \n",
       "9        3.15                  0.22             7.22  1.01     1045  "
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各カラムを抽出する場合は、辞書型のようにカラム名を指定すればOKです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "5    1\n",
       "6    1\n",
       "7    1\n",
       "8    1\n",
       "9    1\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、.valuesと付けると、Pandasの形式からNumpyの形式に変換でき、これをよく用います。\n",
    "理由として、ChainerやScikit-learnのライブラリでは、Numpyの形式で保存された変数を前提に設計されていることが多いため、データの抽出が終わった最後に、.valuesを付けてNumpyの形式に変換しておくと無難です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、入力変数が多い場合など、手動でカラム名を全部指定することが面倒なときは、df.ilocを使用すると、Numpy含めた行列の表現として抽出することができます。\n",
    "\n",
    "例えば、全ての行（:）で１〜２列目（0:2）を抽出したい場合は以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>14.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>14.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>14.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>14.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>13.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Alcohol\n",
       "0      1    14.23\n",
       "1      1    13.20\n",
       "2      1    13.16\n",
       "3      1    14.37\n",
       "4      1    13.24\n",
       "5      1    14.20\n",
       "6      1    14.39\n",
       "7      1    14.06\n",
       "8      1    14.83\n",
       "9      1    13.86"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, 0:2].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力変数と出力変数に分ける"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はワインの成分から等級を分類するといった問題設定であるため、\n",
    "\n",
    "- 入力変数：AlcoholからProlineまでの数値データ\n",
    "- 出力変数：Class\n",
    "\n",
    "を採用します。\n",
    "\n",
    "先ほど紹介したdf.ilocを使用してデータを切り分けましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = df.iloc[:, 1:].values\n",
    "_t = df.iloc[:, 0].values - 1  # chainerのラベル付けは0スタートのため、等級は1からではなく、0から始めておく"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、Chainerで使用する際には、\n",
    "\n",
    "- 実数値：np.float32\n",
    "- 整数値：np.int32\n",
    "\n",
    "に変換しておかないとよくエラーがでるため、事前にこちらへ変換しておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(_x, dtype=np.float32)\n",
    "t = np.array(_t, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ 一時的に保存する変数名に_xのようにアンダーバー（アンダースコア）を付けています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variableに変換\n",
    "Chainerを使う時になかなか難しいのが変数の取り扱いです。  \n",
    "Chainerでは、chainer.Variableで定義されている変数の方に変換してから、モデルの学習用に使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chainer用の変数として宣言\n",
    "x_ch = Variable(x)\n",
    "t_ch = Variable(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを学習させよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル学習の流れは以下のとおりです。\n",
    "\n",
    "> 順伝播の計算　▶　勾配の計算　▶　パラメータの調整\n",
    "\n",
    "これをforでループさせ、評価関数の値が小さくなるように、パラメータをどんどん調整していきます。\n",
    "\n",
    "それでは、学習ループのプログラムを書いていきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000回順伝播と逆伝播を繰り返す\n",
    "for i in range(1000):\n",
    "    \n",
    "    # 勾配情報の初期化（chainerの仕様）\n",
    "    model.cleargrads()\n",
    "    \n",
    "    # 順伝播（評価関数の計算）\n",
    "    loss = model(x_ch, t_ch)\n",
    "    \n",
    "    # 勾配（傾き）の計算 <-　誤差逆伝播法が使用されている\n",
    "    loss.backward()\n",
    "    \n",
    "    # パラメータの調整\n",
    "    optimizer.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本的には最初に提示した流れをforで所定回数繰り返しています。\n",
    "\n",
    "唯一、model.cleargrad()という項があり、これはChainerの仕様なのですが、学習の前に勾配を保存している変数を初期化する必要があります。\n",
    "ただ、こちらは「Chainerの仕様」であるため、特に気にせず、毎回書くものだと思っておいてください。\n",
    "\n",
    "loss = model(x_ch, t_ch)の項では、modelを関数風に呼び出しているため、modelの`__call__`の関数が呼び出されており、評価関数の値が計算されています。\n",
    "\n",
    "そして、その計算した評価関数をもとに、loss.backward()とすると、不思議ですが、model内のパラメータWとbの勾配に関するアトリビュートであるmodel.W.gradや model.b.gradに最新の勾配の値が代入されるようになっています。\n",
    "このあたりは「なぜ？」というよりも「Chainerの仕様だから」と片付けてしまう方が良いと思います。\n",
    "最初に説明した**Define by Run**を実現するための構造であったりします。\n",
    "\n",
    "また、上記の学習のプログラムでは、学習の経過の様子がわからないため、lossを保存するリストを準備してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []  # 追記：訓練結果保存用のリスト\n",
    "for i in range(1000):\n",
    "    model.cleargrads()\n",
    "    loss = model(x_ch, t_ch)\n",
    "    loss.backward()\n",
    "    optimizer.update()\n",
    "    # 追記：プロットするように保存しておく\n",
    "    losses.append(loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで学習した結果を保存できるため、どれくらい評価関数の値（平均二乗誤差）が下がっているかプロットして確認しましょう。\n",
    "\n",
    "プロットには**Matplotlib**を使いましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2ed4573208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 評価関数の値をプロット\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果を確認しよう\n",
    "それでは、まだ学習させることで良くなりそうですが、現状のモデルに対して、どの程度うまくいっているかを目視で確認してみましょう。\n",
    "目視では、定量的な厳密性のある議論ができませんが、そもそも全くうまくいっていないことは瞬時に判断することができるため、最初は目視による確認から入ることがおすすめです。\n",
    "\n",
    "まず、現状のモデルで予測値の計算（推論）を行いましょう。  \n",
    "※ まず１番目のデータに対する計算です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mlp(x_ch[None, 0]).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみに、mlp(x[0])と書かずに、mlp(x[None, 0])と書くことに違和感があると思いますが、Chainerでは一行目にバッチサイズが来ないといけないため、こちらを変換する必要があります。\n",
    "こちらを変更するための書き方は下記のように複数あります。\n",
    "\n",
    "- x[None, 0]\n",
    "- x[0].reshape(1, len(x[0]))\n",
    "- np.array([x[0]])\n",
    "\n",
    "一番シンプルな書き方がx[None, 0]であるため、こちらを使用することをおすすめします。\n",
    "※ PFNの方もこちらの書き方でした。\n",
    "\n",
    "ここで、Chainerで定義したmodelから得られる計算の値はChainerのVariableで得られるため、数値として取り扱うために.dataでnumpyの形式で数値を取り出しています。\n",
    "\n",
    "推論した結果は以下のとおりです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0068161 ,  0.19196826, -0.19878453]], dtype=float32)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらのように、クラスの予測値が出てきてくれるわけではなく、回帰の結果が出力されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = F.softmax(y).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3313936 , 0.39879954, 0.2698068 ]], dtype=float32)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらのように、softmax関数にかけた後に、np.argmaxで確率が最大のインデックスが取得でき、こちらが該当するクラスになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainerを使おう\n",
    "\n",
    "<img src=\"./images/05.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainerとは？\n",
    "先ほどの講義では、実際にカスタマイズすべき点の挙動も確認しながら、Chainerの使い方を見てきましたが、この方法では実務に入る際に問題点があります。\n",
    "\n",
    "- バッチサイズの指定を学習ループのfor文にいちいち書かないといけない（結構めんどくさい）\n",
    "- 検証用データに対する結果を計算するために、追記しないといけない（結構めんどくさい）\n",
    "- ループしている状況を可視化して確認したい際には print などでいちいち確認するためのコードを書かないといけない（これが入るとコードの本質を見失う）\n",
    "- 訓練データと検証データで時系列的にどの程度誤差が下がっているか確認するためのコードも書かないといけない（これを見ないと、オーバーフィッティングに気づけないため必須）\n",
    "- etc…\n",
    "\n",
    "といったように、先ほどまでの手順で使えるようになるのと、実用化を視野に入れるのでは、結構なレベルの差が合ったりします。\n",
    "\n",
    "この部分をサポートしてくれているのがChainerのTrainerです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数のシードを固定 (再現性の確保)\n",
    "np.random.seed(1)\n",
    "\n",
    "# モデルの宣言\n",
    "n_units1, n_units2, n_output = 10, 10, 3\n",
    "mlp = MLP(n_units1, n_units2, n_output)\n",
    "model = L.Classifier(mlp)\n",
    "\n",
    "# optimizerの設定\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasetの設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainerを使用する際は、Variableにデータを変換するといった手順は省略できます。\n",
    "その代わり、データを所定の形式に変換しておく必要があります。\n",
    "\n",
    "メモリに乗り切る程度の小規模なデータの際は、**入力変数と教師データをタプルで１セット**にして、それを**リスト化**しておくことがChainer推奨の方法だそうです。\n",
    "\n",
    "<img src=\"./images/06.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(zip(x, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらのように、zip(x, t)で入力変数と教師データをタプル化した後、それをlistでリスト化します。\n",
    "こちらは毎回同じ記述であるため、このように書くと覚えておいていただければOKです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([1.423e+01, 2.430e+00, 1.560e+01, 1.270e+02, 2.800e+00, 3.060e+00,\n",
       "         2.800e-01, 5.640e+00, 1.040e+00, 1.065e+03], dtype=float32), 0),\n",
       " (array([1.32e+01, 2.14e+00, 1.12e+01, 1.00e+02, 2.65e+00, 2.76e+00,\n",
       "         2.60e-01, 4.38e+00, 1.05e+00, 1.05e+03], dtype=float32), 0),\n",
       " (array([1.316e+01, 2.670e+00, 1.860e+01, 1.010e+02, 2.800e+00, 3.240e+00,\n",
       "         3.000e-01, 5.680e+00, 1.030e+00, 1.185e+03], dtype=float32), 0),\n",
       " (array([1.437e+01, 2.500e+00, 1.680e+01, 1.130e+02, 3.850e+00, 3.490e+00,\n",
       "         2.400e-01, 7.800e+00, 8.600e-01, 1.480e+03], dtype=float32), 0),\n",
       " (array([1.324e+01, 2.870e+00, 2.100e+01, 1.180e+02, 2.800e+00, 2.690e+00,\n",
       "         3.900e-01, 4.320e+00, 1.040e+00, 7.350e+02], dtype=float32), 0),\n",
       " (array([1.42e+01, 2.45e+00, 1.52e+01, 1.12e+02, 3.27e+00, 3.39e+00,\n",
       "         3.40e-01, 6.75e+00, 1.05e+00, 1.45e+03], dtype=float32), 0),\n",
       " (array([1.439e+01, 2.450e+00, 1.460e+01, 9.600e+01, 2.500e+00, 2.520e+00,\n",
       "         3.000e-01, 5.250e+00, 1.020e+00, 1.290e+03], dtype=float32), 0),\n",
       " (array([1.406e+01, 2.610e+00, 1.760e+01, 1.210e+02, 2.600e+00, 2.510e+00,\n",
       "         3.100e-01, 5.050e+00, 1.060e+00, 1.295e+03], dtype=float32), 0),\n",
       " (array([1.483e+01, 2.170e+00, 1.400e+01, 9.700e+01, 2.800e+00, 2.980e+00,\n",
       "         2.900e-01, 5.200e+00, 1.080e+00, 1.045e+03], dtype=float32), 0),\n",
       " (array([1.386e+01, 2.270e+00, 1.600e+01, 9.800e+01, 2.980e+00, 3.150e+00,\n",
       "         2.200e-01, 7.220e+00, 1.010e+00, 1.045e+03], dtype=float32), 0),\n",
       " (array([1.41e+01, 2.30e+00, 1.80e+01, 1.05e+02, 2.95e+00, 3.32e+00,\n",
       "         2.20e-01, 5.75e+00, 1.25e+00, 1.51e+03], dtype=float32), 0),\n",
       " (array([1.412e+01, 2.320e+00, 1.680e+01, 9.500e+01, 2.200e+00, 2.430e+00,\n",
       "         2.600e-01, 5.000e+00, 1.170e+00, 1.280e+03], dtype=float32), 0),\n",
       " (array([1.375e+01, 2.410e+00, 1.600e+01, 8.900e+01, 2.600e+00, 2.760e+00,\n",
       "         2.900e-01, 5.600e+00, 1.150e+00, 1.320e+03], dtype=float32), 0),\n",
       " (array([1.475e+01, 2.390e+00, 1.140e+01, 9.100e+01, 3.100e+00, 3.690e+00,\n",
       "         4.300e-01, 5.400e+00, 1.250e+00, 1.150e+03], dtype=float32), 0),\n",
       " (array([1.438e+01, 2.380e+00, 1.200e+01, 1.020e+02, 3.300e+00, 3.640e+00,\n",
       "         2.900e-01, 7.500e+00, 1.200e+00, 1.547e+03], dtype=float32), 0),\n",
       " (array([1.363e+01, 2.700e+00, 1.720e+01, 1.120e+02, 2.850e+00, 2.910e+00,\n",
       "         3.000e-01, 7.300e+00, 1.280e+00, 1.310e+03], dtype=float32), 0),\n",
       " (array([1.43e+01, 2.72e+00, 2.00e+01, 1.20e+02, 2.80e+00, 3.14e+00,\n",
       "         3.30e-01, 6.20e+00, 1.07e+00, 1.28e+03], dtype=float32), 0),\n",
       " (array([1.383e+01, 2.620e+00, 2.000e+01, 1.150e+02, 2.950e+00, 3.400e+00,\n",
       "         4.000e-01, 6.600e+00, 1.130e+00, 1.130e+03], dtype=float32), 0),\n",
       " (array([1.419e+01, 2.480e+00, 1.650e+01, 1.080e+02, 3.300e+00, 3.930e+00,\n",
       "         3.200e-01, 8.700e+00, 1.230e+00, 1.680e+03], dtype=float32), 0),\n",
       " (array([1.364e+01, 2.560e+00, 1.520e+01, 1.160e+02, 2.700e+00, 3.030e+00,\n",
       "         1.700e-01, 5.100e+00, 9.600e-01, 8.450e+02], dtype=float32), 0),\n",
       " (array([1.406e+01, 2.280e+00, 1.600e+01, 1.260e+02, 3.000e+00, 3.170e+00,\n",
       "         2.400e-01, 5.650e+00, 1.090e+00, 7.800e+02], dtype=float32), 0),\n",
       " (array([1.293e+01, 2.650e+00, 1.860e+01, 1.020e+02, 2.410e+00, 2.410e+00,\n",
       "         2.500e-01, 4.500e+00, 1.030e+00, 7.700e+02], dtype=float32), 0),\n",
       " (array([1.371e+01, 2.360e+00, 1.660e+01, 1.010e+02, 2.610e+00, 2.880e+00,\n",
       "         2.700e-01, 3.800e+00, 1.110e+00, 1.035e+03], dtype=float32), 0),\n",
       " (array([1.285e+01, 2.520e+00, 1.780e+01, 9.500e+01, 2.480e+00, 2.370e+00,\n",
       "         2.600e-01, 3.930e+00, 1.090e+00, 1.015e+03], dtype=float32), 0),\n",
       " (array([1.35e+01, 2.61e+00, 2.00e+01, 9.60e+01, 2.53e+00, 2.61e+00,\n",
       "         2.80e-01, 3.52e+00, 1.12e+00, 8.45e+02], dtype=float32), 0),\n",
       " (array([1.305e+01, 3.220e+00, 2.500e+01, 1.240e+02, 2.630e+00, 2.680e+00,\n",
       "         4.700e-01, 3.580e+00, 1.130e+00, 8.300e+02], dtype=float32), 0),\n",
       " (array([1.339e+01, 2.620e+00, 1.610e+01, 9.300e+01, 2.850e+00, 2.940e+00,\n",
       "         3.400e-01, 4.800e+00, 9.200e-01, 1.195e+03], dtype=float32), 0),\n",
       " (array([1.330e+01, 2.140e+00, 1.700e+01, 9.400e+01, 2.400e+00, 2.190e+00,\n",
       "         2.700e-01, 3.950e+00, 1.020e+00, 1.285e+03], dtype=float32), 0),\n",
       " (array([1.387e+01, 2.800e+00, 1.940e+01, 1.070e+02, 2.950e+00, 2.970e+00,\n",
       "         3.700e-01, 4.500e+00, 1.250e+00, 9.150e+02], dtype=float32), 0),\n",
       " (array([1.402e+01, 2.210e+00, 1.600e+01, 9.600e+01, 2.650e+00, 2.330e+00,\n",
       "         2.600e-01, 4.700e+00, 1.040e+00, 1.035e+03], dtype=float32), 0),\n",
       " (array([1.373e+01, 2.700e+00, 2.250e+01, 1.010e+02, 3.000e+00, 3.250e+00,\n",
       "         2.900e-01, 5.700e+00, 1.190e+00, 1.285e+03], dtype=float32), 0),\n",
       " (array([1.358e+01, 2.360e+00, 1.910e+01, 1.060e+02, 2.860e+00, 3.190e+00,\n",
       "         2.200e-01, 6.900e+00, 1.090e+00, 1.515e+03], dtype=float32), 0),\n",
       " (array([1.368e+01, 2.360e+00, 1.720e+01, 1.040e+02, 2.420e+00, 2.690e+00,\n",
       "         4.200e-01, 3.840e+00, 1.230e+00, 9.900e+02], dtype=float32), 0),\n",
       " (array([1.376e+01, 2.700e+00, 1.950e+01, 1.320e+02, 2.950e+00, 2.740e+00,\n",
       "         5.000e-01, 5.400e+00, 1.250e+00, 1.235e+03], dtype=float32), 0),\n",
       " (array([1.351e+01, 2.650e+00, 1.900e+01, 1.100e+02, 2.350e+00, 2.530e+00,\n",
       "         2.900e-01, 4.200e+00, 1.100e+00, 1.095e+03], dtype=float32), 0),\n",
       " (array([1.348e+01, 2.410e+00, 2.050e+01, 1.000e+02, 2.700e+00, 2.980e+00,\n",
       "         2.600e-01, 5.100e+00, 1.040e+00, 9.200e+02], dtype=float32), 0),\n",
       " (array([1.328e+01, 2.840e+00, 1.550e+01, 1.100e+02, 2.600e+00, 2.680e+00,\n",
       "         3.400e-01, 4.600e+00, 1.090e+00, 8.800e+02], dtype=float32), 0),\n",
       " (array([1.305e+01, 2.550e+00, 1.800e+01, 9.800e+01, 2.450e+00, 2.430e+00,\n",
       "         2.900e-01, 4.250e+00, 1.120e+00, 1.105e+03], dtype=float32), 0),\n",
       " (array([1.307e+01, 2.100e+00, 1.550e+01, 9.800e+01, 2.400e+00, 2.640e+00,\n",
       "         2.800e-01, 3.700e+00, 1.180e+00, 1.020e+03], dtype=float32), 0),\n",
       " (array([1.422e+01, 2.510e+00, 1.320e+01, 1.280e+02, 3.000e+00, 3.040e+00,\n",
       "         2.000e-01, 5.100e+00, 8.900e-01, 7.600e+02], dtype=float32), 0),\n",
       " (array([1.356e+01, 2.310e+00, 1.620e+01, 1.170e+02, 3.150e+00, 3.290e+00,\n",
       "         3.400e-01, 6.130e+00, 9.500e-01, 7.950e+02], dtype=float32), 0),\n",
       " (array([1.341e+01, 2.120e+00, 1.880e+01, 9.000e+01, 2.450e+00, 2.680e+00,\n",
       "         2.700e-01, 4.280e+00, 9.100e-01, 1.035e+03], dtype=float32), 0),\n",
       " (array([1.388e+01, 2.590e+00, 1.500e+01, 1.010e+02, 3.250e+00, 3.560e+00,\n",
       "         1.700e-01, 5.430e+00, 8.800e-01, 1.095e+03], dtype=float32), 0),\n",
       " (array([1.324e+01, 2.290e+00, 1.750e+01, 1.030e+02, 2.640e+00, 2.630e+00,\n",
       "         3.200e-01, 4.360e+00, 8.200e-01, 6.800e+02], dtype=float32), 0),\n",
       " (array([1.305e+01, 2.100e+00, 1.700e+01, 1.070e+02, 3.000e+00, 3.000e+00,\n",
       "         2.800e-01, 5.040e+00, 8.800e-01, 8.850e+02], dtype=float32), 0),\n",
       " (array([1.421e+01, 2.440e+00, 1.890e+01, 1.110e+02, 2.850e+00, 2.650e+00,\n",
       "         3.000e-01, 5.240e+00, 8.700e-01, 1.080e+03], dtype=float32), 0),\n",
       " (array([1.438e+01, 2.280e+00, 1.600e+01, 1.020e+02, 3.250e+00, 3.170e+00,\n",
       "         2.700e-01, 4.900e+00, 1.040e+00, 1.065e+03], dtype=float32), 0),\n",
       " (array([1.39e+01, 2.12e+00, 1.60e+01, 1.01e+02, 3.10e+00, 3.39e+00,\n",
       "         2.10e-01, 6.10e+00, 9.10e-01, 9.85e+02], dtype=float32), 0),\n",
       " (array([1.41e+01, 2.40e+00, 1.88e+01, 1.03e+02, 2.75e+00, 2.92e+00,\n",
       "         3.20e-01, 6.20e+00, 1.07e+00, 1.06e+03], dtype=float32), 0),\n",
       " (array([1.394e+01, 2.270e+00, 1.740e+01, 1.080e+02, 2.880e+00, 3.540e+00,\n",
       "         3.200e-01, 8.900e+00, 1.120e+00, 1.260e+03], dtype=float32), 0),\n",
       " (array([1.305e+01, 2.040e+00, 1.240e+01, 9.200e+01, 2.720e+00, 3.270e+00,\n",
       "         1.700e-01, 7.200e+00, 1.120e+00, 1.150e+03], dtype=float32), 0),\n",
       " (array([1.383e+01, 2.600e+00, 1.720e+01, 9.400e+01, 2.450e+00, 2.990e+00,\n",
       "         2.200e-01, 5.600e+00, 1.240e+00, 1.265e+03], dtype=float32), 0),\n",
       " (array([1.382e+01, 2.420e+00, 1.400e+01, 1.110e+02, 3.880e+00, 3.740e+00,\n",
       "         3.200e-01, 7.050e+00, 1.010e+00, 1.190e+03], dtype=float32), 0),\n",
       " (array([1.377e+01, 2.680e+00, 1.710e+01, 1.150e+02, 3.000e+00, 2.790e+00,\n",
       "         3.900e-01, 6.300e+00, 1.130e+00, 1.375e+03], dtype=float32), 0),\n",
       " (array([1.374e+01, 2.250e+00, 1.640e+01, 1.180e+02, 2.600e+00, 2.900e+00,\n",
       "         2.100e-01, 5.850e+00, 9.200e-01, 1.060e+03], dtype=float32), 0),\n",
       " (array([1.356e+01, 2.460e+00, 2.050e+01, 1.160e+02, 2.960e+00, 2.780e+00,\n",
       "         2.000e-01, 6.250e+00, 9.800e-01, 1.120e+03], dtype=float32), 0),\n",
       " (array([1.422e+01, 2.300e+00, 1.630e+01, 1.180e+02, 3.200e+00, 3.000e+00,\n",
       "         2.600e-01, 6.380e+00, 9.400e-01, 9.700e+02], dtype=float32), 0),\n",
       " (array([1.329e+01, 2.680e+00, 1.680e+01, 1.020e+02, 3.000e+00, 3.230e+00,\n",
       "         3.100e-01, 6.000e+00, 1.070e+00, 1.270e+03], dtype=float32), 0),\n",
       " (array([1.372e+01, 2.500e+00, 1.670e+01, 1.080e+02, 3.400e+00, 3.670e+00,\n",
       "         1.900e-01, 6.800e+00, 8.900e-01, 1.285e+03], dtype=float32), 0),\n",
       " (array([1.237e+01, 1.360e+00, 1.060e+01, 8.800e+01, 1.980e+00, 5.700e-01,\n",
       "         2.800e-01, 1.950e+00, 1.050e+00, 5.200e+02], dtype=float32), 1),\n",
       " (array([1.233e+01, 2.280e+00, 1.600e+01, 1.010e+02, 2.050e+00, 1.090e+00,\n",
       "         6.300e-01, 3.270e+00, 1.250e+00, 6.800e+02], dtype=float32), 1),\n",
       " (array([ 12.64,   2.02,  16.8 , 100.  ,   2.02,   1.41,   0.53,   5.75,\n",
       "           0.98, 450.  ], dtype=float32), 1),\n",
       " (array([1.367e+01, 1.920e+00, 1.800e+01, 9.400e+01, 2.100e+00, 1.790e+00,\n",
       "         3.200e-01, 3.800e+00, 1.230e+00, 6.300e+02], dtype=float32), 1),\n",
       " (array([1.237e+01, 2.160e+00, 1.900e+01, 8.700e+01, 3.500e+00, 3.100e+00,\n",
       "         1.900e-01, 4.450e+00, 1.220e+00, 4.200e+02], dtype=float32), 1),\n",
       " (array([ 12.17,   2.53,  19.  , 104.  ,   1.89,   1.75,   0.45,   2.95,\n",
       "           1.45, 355.  ], dtype=float32), 1),\n",
       " (array([1.237e+01, 2.560e+00, 1.810e+01, 9.800e+01, 2.420e+00, 2.650e+00,\n",
       "         3.700e-01, 4.600e+00, 1.190e+00, 6.780e+02], dtype=float32), 1),\n",
       " (array([1.311e+01, 1.700e+00, 1.500e+01, 7.800e+01, 2.980e+00, 3.180e+00,\n",
       "         2.600e-01, 5.300e+00, 1.120e+00, 5.020e+02], dtype=float32), 1),\n",
       " (array([1.237e+01, 1.920e+00, 1.960e+01, 7.800e+01, 2.110e+00, 2.000e+00,\n",
       "         2.700e-01, 4.680e+00, 1.120e+00, 5.100e+02], dtype=float32), 1),\n",
       " (array([1.334e+01, 2.360e+00, 1.700e+01, 1.100e+02, 2.530e+00, 1.300e+00,\n",
       "         5.500e-01, 3.170e+00, 1.020e+00, 7.500e+02], dtype=float32), 1),\n",
       " (array([1.221e+01, 1.750e+00, 1.680e+01, 1.510e+02, 1.850e+00, 1.280e+00,\n",
       "         1.400e-01, 2.850e+00, 1.280e+00, 7.180e+02], dtype=float32), 1),\n",
       " (array([1.229e+01, 2.210e+00, 2.040e+01, 1.030e+02, 1.100e+00, 1.020e+00,\n",
       "         3.700e-01, 3.050e+00, 9.060e-01, 8.700e+02], dtype=float32), 1),\n",
       " (array([1.386e+01, 2.670e+00, 2.500e+01, 8.600e+01, 2.950e+00, 2.860e+00,\n",
       "         2.100e-01, 3.380e+00, 1.360e+00, 4.100e+02], dtype=float32), 1),\n",
       " (array([1.349e+01, 2.240e+00, 2.400e+01, 8.700e+01, 1.880e+00, 1.840e+00,\n",
       "         2.700e-01, 3.740e+00, 9.800e-01, 4.720e+02], dtype=float32), 1),\n",
       " (array([1.299e+01, 2.600e+00, 3.000e+01, 1.390e+02, 3.300e+00, 2.890e+00,\n",
       "         2.100e-01, 3.350e+00, 1.310e+00, 9.850e+02], dtype=float32), 1),\n",
       " (array([1.196e+01, 2.300e+00, 2.100e+01, 1.010e+02, 3.380e+00, 2.140e+00,\n",
       "         1.300e-01, 3.210e+00, 9.900e-01, 8.860e+02], dtype=float32), 1),\n",
       " (array([1.166e+01, 1.920e+00, 1.600e+01, 9.700e+01, 1.610e+00, 1.570e+00,\n",
       "         3.400e-01, 3.800e+00, 1.230e+00, 4.280e+02], dtype=float32), 1),\n",
       " (array([1.303e+01, 1.710e+00, 1.600e+01, 8.600e+01, 1.950e+00, 2.030e+00,\n",
       "         2.400e-01, 4.600e+00, 1.190e+00, 3.920e+02], dtype=float32), 1),\n",
       " (array([1.184e+01, 2.230e+00, 1.800e+01, 1.120e+02, 1.720e+00, 1.320e+00,\n",
       "         4.300e-01, 2.650e+00, 9.600e-01, 5.000e+02], dtype=float32), 1),\n",
       " (array([1.233e+01, 1.950e+00, 1.480e+01, 1.360e+02, 1.900e+00, 1.850e+00,\n",
       "         3.500e-01, 3.400e+00, 1.060e+00, 7.500e+02], dtype=float32), 1),\n",
       " (array([1.27e+01, 2.40e+00, 2.30e+01, 1.01e+02, 2.83e+00, 2.55e+00,\n",
       "         4.30e-01, 2.57e+00, 1.19e+00, 4.63e+02], dtype=float32), 1),\n",
       " (array([ 12.  ,   2.  ,  19.  ,  86.  ,   2.42,   2.26,   0.3 ,   2.5 ,\n",
       "           1.38, 278.  ], dtype=float32), 1),\n",
       " (array([1.272e+01, 2.200e+00, 1.880e+01, 8.600e+01, 2.200e+00, 2.530e+00,\n",
       "         2.600e-01, 3.900e+00, 1.160e+00, 7.140e+02], dtype=float32), 1),\n",
       " (array([1.208e+01, 2.510e+00, 2.400e+01, 7.800e+01, 2.000e+00, 1.580e+00,\n",
       "         4.000e-01, 2.200e+00, 1.310e+00, 6.300e+02], dtype=float32), 1),\n",
       " (array([ 13.05,   2.32,  22.5 ,  85.  ,   1.65,   1.59,   0.61,   4.8 ,\n",
       "           0.84, 515.  ], dtype=float32), 1),\n",
       " (array([1.184e+01, 2.580e+00, 1.800e+01, 9.400e+01, 2.200e+00, 2.210e+00,\n",
       "         2.200e-01, 3.050e+00, 7.900e-01, 5.200e+02], dtype=float32), 1),\n",
       " (array([1.267e+01, 2.240e+00, 1.800e+01, 9.900e+01, 2.200e+00, 1.940e+00,\n",
       "         3.000e-01, 2.620e+00, 1.230e+00, 4.500e+02], dtype=float32), 1),\n",
       " (array([1.216e+01, 2.310e+00, 2.280e+01, 9.000e+01, 1.780e+00, 1.690e+00,\n",
       "         4.300e-01, 2.450e+00, 1.330e+00, 4.950e+02], dtype=float32), 1),\n",
       " (array([1.165e+01, 2.620e+00, 2.600e+01, 8.800e+01, 1.920e+00, 1.610e+00,\n",
       "         4.000e-01, 2.600e+00, 1.360e+00, 5.620e+02], dtype=float32), 1),\n",
       " (array([1.164e+01, 2.460e+00, 2.160e+01, 8.400e+01, 1.950e+00, 1.690e+00,\n",
       "         4.800e-01, 2.800e+00, 1.000e+00, 6.800e+02], dtype=float32), 1),\n",
       " (array([1.208e+01, 2.300e+00, 2.360e+01, 7.000e+01, 2.200e+00, 1.590e+00,\n",
       "         4.200e-01, 1.740e+00, 1.070e+00, 6.250e+02], dtype=float32), 1),\n",
       " (array([ 12.08,   2.32,  18.5 ,  81.  ,   1.6 ,   1.5 ,   0.52,   2.4 ,\n",
       "           1.08, 480.  ], dtype=float32), 1),\n",
       " (array([ 12.  ,   2.42,  22.  ,  86.  ,   1.45,   1.25,   0.5 ,   3.6 ,\n",
       "           1.05, 450.  ], dtype=float32), 1),\n",
       " (array([ 12.69,   2.26,  20.7 ,  80.  ,   1.38,   1.46,   0.58,   3.05,\n",
       "           0.96, 495.  ], dtype=float32), 1),\n",
       " (array([1.229e+01, 2.220e+00, 1.800e+01, 8.800e+01, 2.450e+00, 2.250e+00,\n",
       "         2.500e-01, 2.150e+00, 1.150e+00, 2.900e+02], dtype=float32), 1),\n",
       " (array([1.162e+01, 2.280e+00, 1.800e+01, 9.800e+01, 3.020e+00, 2.260e+00,\n",
       "         1.700e-01, 3.250e+00, 1.160e+00, 3.450e+02], dtype=float32), 1),\n",
       " (array([1.247e+01, 2.200e+00, 1.900e+01, 1.620e+02, 2.500e+00, 2.270e+00,\n",
       "         3.200e-01, 2.600e+00, 1.160e+00, 9.370e+02], dtype=float32), 1),\n",
       " (array([1.181e+01, 2.740e+00, 2.150e+01, 1.340e+02, 1.600e+00, 9.900e-01,\n",
       "         1.400e-01, 2.500e+00, 9.500e-01, 6.250e+02], dtype=float32), 1),\n",
       " (array([1.229e+01, 1.980e+00, 1.600e+01, 8.500e+01, 2.550e+00, 2.500e+00,\n",
       "         2.900e-01, 2.900e+00, 1.230e+00, 4.280e+02], dtype=float32), 1),\n",
       " (array([1.237e+01, 2.100e+00, 1.850e+01, 8.800e+01, 3.520e+00, 3.750e+00,\n",
       "         2.400e-01, 4.500e+00, 1.040e+00, 6.600e+02], dtype=float32), 1),\n",
       " (array([ 12.29,   2.21,  18.  ,  88.  ,   2.85,   2.99,   0.45,   2.3 ,\n",
       "           1.42, 406.  ], dtype=float32), 1),\n",
       " (array([1.208e+01, 1.700e+00, 1.750e+01, 9.700e+01, 2.230e+00, 2.170e+00,\n",
       "         2.600e-01, 3.300e+00, 1.270e+00, 7.100e+02], dtype=float32), 1),\n",
       " (array([1.26e+01, 1.90e+00, 1.85e+01, 8.80e+01, 1.45e+00, 1.36e+00,\n",
       "         2.90e-01, 2.45e+00, 1.04e+00, 5.62e+02], dtype=float32), 1),\n",
       " (array([1.234e+01, 2.460e+00, 2.100e+01, 9.800e+01, 2.560e+00, 2.110e+00,\n",
       "         3.400e-01, 2.800e+00, 8.000e-01, 4.380e+02], dtype=float32), 1),\n",
       " (array([1.182e+01, 1.880e+00, 1.950e+01, 8.600e+01, 2.500e+00, 1.640e+00,\n",
       "         3.700e-01, 2.060e+00, 9.400e-01, 4.150e+02], dtype=float32), 1),\n",
       " (array([1.251e+01, 1.980e+00, 2.050e+01, 8.500e+01, 2.200e+00, 1.920e+00,\n",
       "         3.200e-01, 2.940e+00, 1.040e+00, 6.720e+02], dtype=float32), 1),\n",
       " (array([ 12.42,   2.27,  22.  ,  90.  ,   1.68,   1.84,   0.66,   2.7 ,\n",
       "           0.86, 315.  ], dtype=float32), 1),\n",
       " (array([1.225e+01, 2.120e+00, 1.900e+01, 8.000e+01, 1.650e+00, 2.030e+00,\n",
       "         3.700e-01, 3.400e+00, 1.000e+00, 5.100e+02], dtype=float32), 1),\n",
       " (array([1.272e+01, 2.280e+00, 2.250e+01, 8.400e+01, 1.380e+00, 1.760e+00,\n",
       "         4.800e-01, 3.300e+00, 8.800e-01, 4.880e+02], dtype=float32), 1),\n",
       " (array([ 12.22,   1.94,  19.  ,  92.  ,   2.36,   2.04,   0.39,   2.7 ,\n",
       "           0.86, 312.  ], dtype=float32), 1),\n",
       " (array([1.161e+01, 2.700e+00, 2.000e+01, 9.400e+01, 2.740e+00, 2.920e+00,\n",
       "         2.900e-01, 2.650e+00, 9.600e-01, 6.800e+02], dtype=float32), 1),\n",
       " (array([1.146e+01, 1.820e+00, 1.950e+01, 1.070e+02, 3.180e+00, 2.580e+00,\n",
       "         2.400e-01, 2.900e+00, 7.500e-01, 5.620e+02], dtype=float32), 1),\n",
       " (array([1.252e+01, 2.170e+00, 2.100e+01, 8.800e+01, 2.550e+00, 2.270e+00,\n",
       "         2.600e-01, 2.000e+00, 9.000e-01, 3.250e+02], dtype=float32), 1),\n",
       " (array([1.176e+01, 2.920e+00, 2.000e+01, 1.030e+02, 1.750e+00, 2.030e+00,\n",
       "         6.000e-01, 3.800e+00, 1.230e+00, 6.070e+02], dtype=float32), 1),\n",
       " (array([1.141e+01, 2.500e+00, 2.100e+01, 8.800e+01, 2.480e+00, 2.010e+00,\n",
       "         4.200e-01, 3.080e+00, 1.100e+00, 4.340e+02], dtype=float32), 1),\n",
       " (array([ 12.08,   2.5 ,  22.5 ,  84.  ,   2.56,   2.29,   0.43,   2.9 ,\n",
       "           0.93, 385.  ], dtype=float32), 1),\n",
       " (array([ 11.03,   2.2 ,  21.5 ,  85.  ,   2.46,   2.17,   0.52,   1.9 ,\n",
       "           1.71, 407.  ], dtype=float32), 1),\n",
       " (array([1.182e+01, 1.990e+00, 2.080e+01, 8.600e+01, 1.980e+00, 1.600e+00,\n",
       "         3.000e-01, 1.950e+00, 9.500e-01, 4.950e+02], dtype=float32), 1),\n",
       " (array([1.242e+01, 2.190e+00, 2.250e+01, 1.080e+02, 2.000e+00, 2.090e+00,\n",
       "         3.400e-01, 2.060e+00, 1.060e+00, 3.450e+02], dtype=float32), 1),\n",
       " (array([ 12.77,   1.98,  16.  ,  80.  ,   1.63,   1.25,   0.43,   3.4 ,\n",
       "           0.7 , 372.  ], dtype=float32), 1),\n",
       " (array([1.20e+01, 2.00e+00, 1.90e+01, 8.70e+01, 2.00e+00, 1.64e+00,\n",
       "         3.70e-01, 1.28e+00, 9.30e-01, 5.64e+02], dtype=float32), 1),\n",
       " (array([1.145e+01, 2.420e+00, 2.000e+01, 9.600e+01, 2.900e+00, 2.790e+00,\n",
       "         3.200e-01, 3.250e+00, 8.000e-01, 6.250e+02], dtype=float32), 1),\n",
       " (array([ 11.56,   3.23,  28.5 , 119.  ,   3.18,   5.08,   0.47,   6.  ,\n",
       "           0.93, 465.  ], dtype=float32), 1),\n",
       " (array([ 12.42,   2.73,  26.5 , 102.  ,   2.2 ,   2.13,   0.43,   2.08,\n",
       "           0.92, 365.  ], dtype=float32), 1),\n",
       " (array([1.305e+01, 2.130e+00, 2.150e+01, 8.600e+01, 2.620e+00, 2.650e+00,\n",
       "         3.000e-01, 2.600e+00, 7.300e-01, 3.800e+02], dtype=float32), 1),\n",
       " (array([1.187e+01, 2.390e+00, 2.100e+01, 8.200e+01, 2.860e+00, 3.030e+00,\n",
       "         2.100e-01, 2.800e+00, 7.500e-01, 3.800e+02], dtype=float32), 1),\n",
       " (array([1.207e+01, 2.170e+00, 2.100e+01, 8.500e+01, 2.600e+00, 2.650e+00,\n",
       "         3.700e-01, 2.760e+00, 8.600e-01, 3.780e+02], dtype=float32), 1),\n",
       " (array([ 12.43,   2.29,  21.5 ,  86.  ,   2.74,   3.15,   0.39,   3.94,\n",
       "           0.69, 352.  ], dtype=float32), 1),\n",
       " (array([ 11.79,   2.78,  28.5 ,  92.  ,   2.13,   2.24,   0.58,   3.  ,\n",
       "           0.97, 466.  ], dtype=float32), 1),\n",
       " (array([ 12.37,   2.3 ,  24.5 ,  88.  ,   2.22,   2.45,   0.4 ,   2.12,\n",
       "           0.89, 342.  ], dtype=float32), 1),\n",
       " (array([1.204e+01, 2.380e+00, 2.200e+01, 8.000e+01, 2.100e+00, 1.750e+00,\n",
       "         4.200e-01, 2.600e+00, 7.900e-01, 5.800e+02], dtype=float32), 1),\n",
       " (array([1.286e+01, 2.320e+00, 1.800e+01, 1.220e+02, 1.510e+00, 1.250e+00,\n",
       "         2.100e-01, 4.100e+00, 7.600e-01, 6.300e+02], dtype=float32), 2),\n",
       " (array([1.288e+01, 2.400e+00, 2.000e+01, 1.040e+02, 1.300e+00, 1.220e+00,\n",
       "         2.400e-01, 5.400e+00, 7.400e-01, 5.300e+02], dtype=float32), 2),\n",
       " (array([1.281e+01, 2.400e+00, 2.400e+01, 9.800e+01, 1.150e+00, 1.090e+00,\n",
       "         2.700e-01, 5.700e+00, 6.600e-01, 5.600e+02], dtype=float32), 2),\n",
       " (array([1.27e+01, 2.36e+00, 2.15e+01, 1.06e+02, 1.70e+00, 1.20e+00,\n",
       "         1.70e-01, 5.00e+00, 7.80e-01, 6.00e+02], dtype=float32), 2),\n",
       " (array([1.251e+01, 2.250e+00, 1.750e+01, 8.500e+01, 2.000e+00, 5.800e-01,\n",
       "         6.000e-01, 5.450e+00, 7.500e-01, 6.500e+02], dtype=float32), 2),\n",
       " (array([1.26e+01, 2.20e+00, 1.85e+01, 9.40e+01, 1.62e+00, 6.60e-01,\n",
       "         6.30e-01, 7.10e+00, 7.30e-01, 6.95e+02], dtype=float32), 2),\n",
       " (array([1.225e+01, 2.540e+00, 2.100e+01, 8.900e+01, 1.380e+00, 4.700e-01,\n",
       "         5.300e-01, 3.850e+00, 7.500e-01, 7.200e+02], dtype=float32), 2),\n",
       " (array([ 12.53,   2.64,  25.  ,  96.  ,   1.79,   0.6 ,   0.63,   5.  ,\n",
       "           0.82, 515.  ], dtype=float32), 2),\n",
       " (array([1.349e+01, 2.190e+00, 1.950e+01, 8.800e+01, 1.620e+00, 4.800e-01,\n",
       "         5.800e-01, 5.700e+00, 8.100e-01, 5.800e+02], dtype=float32), 2),\n",
       " (array([1.284e+01, 2.610e+00, 2.400e+01, 1.010e+02, 2.320e+00, 6.000e-01,\n",
       "         5.300e-01, 4.920e+00, 8.900e-01, 5.900e+02], dtype=float32), 2),\n",
       " (array([1.293e+01, 2.700e+00, 2.100e+01, 9.600e+01, 1.540e+00, 5.000e-01,\n",
       "         5.300e-01, 4.600e+00, 7.700e-01, 6.000e+02], dtype=float32), 2),\n",
       " (array([1.336e+01, 2.350e+00, 2.000e+01, 8.900e+01, 1.400e+00, 5.000e-01,\n",
       "         3.700e-01, 5.600e+00, 7.000e-01, 7.800e+02], dtype=float32), 2),\n",
       " (array([1.352e+01, 2.720e+00, 2.350e+01, 9.700e+01, 1.550e+00, 5.200e-01,\n",
       "         5.000e-01, 4.350e+00, 8.900e-01, 5.200e+02], dtype=float32), 2),\n",
       " (array([1.362e+01, 2.350e+00, 2.000e+01, 9.200e+01, 2.000e+00, 8.000e-01,\n",
       "         4.700e-01, 4.400e+00, 9.100e-01, 5.500e+02], dtype=float32), 2),\n",
       " (array([1.225e+01, 2.200e+00, 1.850e+01, 1.120e+02, 1.380e+00, 7.800e-01,\n",
       "         2.900e-01, 8.210e+00, 6.500e-01, 8.550e+02], dtype=float32), 2),\n",
       " (array([1.316e+01, 2.150e+00, 2.100e+01, 1.020e+02, 1.500e+00, 5.500e-01,\n",
       "         4.300e-01, 4.000e+00, 6.000e-01, 8.300e+02], dtype=float32), 2),\n",
       " (array([1.388e+01, 2.230e+00, 2.000e+01, 8.000e+01, 9.800e-01, 3.400e-01,\n",
       "         4.000e-01, 4.900e+00, 5.800e-01, 4.150e+02], dtype=float32), 2),\n",
       " (array([1.287e+01, 2.480e+00, 2.150e+01, 8.600e+01, 1.700e+00, 6.500e-01,\n",
       "         4.700e-01, 7.650e+00, 5.400e-01, 6.250e+02], dtype=float32), 2),\n",
       " (array([1.332e+01, 2.380e+00, 2.150e+01, 9.200e+01, 1.930e+00, 7.600e-01,\n",
       "         4.500e-01, 8.420e+00, 5.500e-01, 6.500e+02], dtype=float32), 2),\n",
       " (array([1.308e+01, 2.360e+00, 2.150e+01, 1.130e+02, 1.410e+00, 1.390e+00,\n",
       "         3.400e-01, 9.400e+00, 5.700e-01, 5.500e+02], dtype=float32), 2),\n",
       " (array([1.35e+01, 2.62e+00, 2.40e+01, 1.23e+02, 1.40e+00, 1.57e+00,\n",
       "         2.20e-01, 8.60e+00, 5.90e-01, 5.00e+02], dtype=float32), 2),\n",
       " (array([1.279e+01, 2.480e+00, 2.200e+01, 1.120e+02, 1.480e+00, 1.360e+00,\n",
       "         2.400e-01, 1.080e+01, 4.800e-01, 4.800e+02], dtype=float32), 2),\n",
       " (array([1.311e+01, 2.750e+00, 2.550e+01, 1.160e+02, 2.200e+00, 1.280e+00,\n",
       "         2.600e-01, 7.100e+00, 6.100e-01, 4.250e+02], dtype=float32), 2),\n",
       " (array([1.323e+01, 2.280e+00, 1.850e+01, 9.800e+01, 1.800e+00, 8.300e-01,\n",
       "         6.100e-01, 1.052e+01, 5.600e-01, 6.750e+02], dtype=float32), 2),\n",
       " (array([1.258e+01, 2.100e+00, 2.000e+01, 1.030e+02, 1.480e+00, 5.800e-01,\n",
       "         5.300e-01, 7.600e+00, 5.800e-01, 6.400e+02], dtype=float32), 2),\n",
       " (array([1.317e+01, 2.320e+00, 2.200e+01, 9.300e+01, 1.740e+00, 6.300e-01,\n",
       "         6.100e-01, 7.900e+00, 6.000e-01, 7.250e+02], dtype=float32), 2),\n",
       " (array([ 13.84,   2.38,  19.5 ,  89.  ,   1.8 ,   0.83,   0.48,   9.01,\n",
       "           0.57, 480.  ], dtype=float32), 2),\n",
       " (array([1.245e+01, 2.640e+00, 2.700e+01, 9.700e+01, 1.900e+00, 5.800e-01,\n",
       "         6.300e-01, 7.500e+00, 6.700e-01, 8.800e+02], dtype=float32), 2),\n",
       " (array([1.434e+01, 2.700e+00, 2.500e+01, 9.800e+01, 2.800e+00, 1.310e+00,\n",
       "         5.300e-01, 1.300e+01, 5.700e-01, 6.600e+02], dtype=float32), 2),\n",
       " (array([1.348e+01, 2.640e+00, 2.250e+01, 8.900e+01, 2.600e+00, 1.100e+00,\n",
       "         5.200e-01, 1.175e+01, 5.700e-01, 6.200e+02], dtype=float32), 2),\n",
       " (array([1.236e+01, 2.380e+00, 2.100e+01, 8.800e+01, 2.300e+00, 9.200e-01,\n",
       "         5.000e-01, 7.650e+00, 5.600e-01, 5.200e+02], dtype=float32), 2),\n",
       " (array([1.369e+01, 2.540e+00, 2.000e+01, 1.070e+02, 1.830e+00, 5.600e-01,\n",
       "         5.000e-01, 5.880e+00, 9.600e-01, 6.800e+02], dtype=float32), 2),\n",
       " (array([ 12.85,   2.58,  22.  , 106.  ,   1.65,   0.6 ,   0.6 ,   5.58,\n",
       "           0.87, 570.  ], dtype=float32), 2),\n",
       " (array([1.296e+01, 2.350e+00, 1.850e+01, 1.060e+02, 1.390e+00, 7.000e-01,\n",
       "         4.000e-01, 5.280e+00, 6.800e-01, 6.750e+02], dtype=float32), 2),\n",
       " (array([1.378e+01, 2.300e+00, 2.200e+01, 9.000e+01, 1.350e+00, 6.800e-01,\n",
       "         4.100e-01, 9.580e+00, 7.000e-01, 6.150e+02], dtype=float32), 2),\n",
       " (array([1.373e+01, 2.260e+00, 2.250e+01, 8.800e+01, 1.280e+00, 4.700e-01,\n",
       "         5.200e-01, 6.620e+00, 7.800e-01, 5.200e+02], dtype=float32), 2),\n",
       " (array([1.345e+01, 2.600e+00, 2.300e+01, 1.110e+02, 1.700e+00, 9.200e-01,\n",
       "         4.300e-01, 1.068e+01, 8.500e-01, 6.950e+02], dtype=float32), 2),\n",
       " (array([1.282e+01, 2.300e+00, 1.950e+01, 8.800e+01, 1.480e+00, 6.600e-01,\n",
       "         4.000e-01, 1.026e+01, 7.200e-01, 6.850e+02], dtype=float32), 2),\n",
       " (array([1.358e+01, 2.690e+00, 2.450e+01, 1.050e+02, 1.550e+00, 8.400e-01,\n",
       "         3.900e-01, 8.660e+00, 7.400e-01, 7.500e+02], dtype=float32), 2),\n",
       " (array([1.34e+01, 2.86e+00, 2.50e+01, 1.12e+02, 1.98e+00, 9.60e-01,\n",
       "         2.70e-01, 8.50e+00, 6.70e-01, 6.30e+02], dtype=float32), 2),\n",
       " (array([1.22e+01, 2.32e+00, 1.90e+01, 9.60e+01, 1.25e+00, 4.90e-01,\n",
       "         4.00e-01, 5.50e+00, 6.60e-01, 5.10e+02], dtype=float32), 2),\n",
       " (array([ 12.77    ,   2.28    ,  19.5     ,  86.      ,   1.39    ,\n",
       "           0.51    ,   0.48    ,   9.899999,   0.57    , 470.      ],\n",
       "        dtype=float32), 2),\n",
       " (array([1.416e+01, 2.480e+00, 2.000e+01, 9.100e+01, 1.680e+00, 7.000e-01,\n",
       "         4.400e-01, 9.700e+00, 6.200e-01, 6.600e+02], dtype=float32), 2),\n",
       " (array([1.371e+01, 2.450e+00, 2.050e+01, 9.500e+01, 1.680e+00, 6.100e-01,\n",
       "         5.200e-01, 7.700e+00, 6.400e-01, 7.400e+02], dtype=float32), 2),\n",
       " (array([1.34e+01, 2.48e+00, 2.30e+01, 1.02e+02, 1.80e+00, 7.50e-01,\n",
       "         4.30e-01, 7.30e+00, 7.00e-01, 7.50e+02], dtype=float32), 2),\n",
       " (array([1.327e+01, 2.260e+00, 2.000e+01, 1.200e+02, 1.590e+00, 6.900e-01,\n",
       "         4.300e-01, 1.020e+01, 5.900e-01, 8.350e+02], dtype=float32), 2),\n",
       " (array([1.317e+01, 2.370e+00, 2.000e+01, 1.200e+02, 1.650e+00, 6.800e-01,\n",
       "         5.300e-01, 9.300e+00, 6.000e-01, 8.400e+02], dtype=float32), 2),\n",
       " (array([ 14.13,   2.74,  24.5 ,  96.  ,   2.05,   0.76,   0.56,   9.2 ,\n",
       "           0.61, 560.  ], dtype=float32), 2)]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練データと検証データに分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainerではデータセットをランダムに分割に関する関数が準備されており、chainer.datasets.split_dataset_randomです。  \n",
    "※ 詳しくは[こちら](https://docs.chainer.org/en/stable/reference/generated/chainer.datasets.split_dataset_random.html#chainer.datasets.split_dataset_random)のリファレンス参照\n",
    "\n",
    "<img src=\"./images/07.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prameterには、dataset（先ほど作成した形式）とfirst_sizeと指定されています。\n",
    "\n",
    "first_sizeでは訓練データのサイズを指定するのですが、こちらの指定をする際に、全体の70%を訓練データにしようと決めておくと記述が簡単かつ汎用性の高いプログラムになります。\n",
    "\n",
    "全体のサイズを取得する時にはlen()が便利です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split_dataset_randomを使用したtrainとtestの分割は以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer.datasets import split_dataset_random\n",
    "\n",
    "n_train = int( len(dataset) * 0.7 )  # 訓練データのサイズ\n",
    "train, test = split_dataset_random(dataset, n_train, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_trainを計算する際に、intと付けていますが、サイズは整数値しか受け付けないため、少数が出た際にはintによって小数値の切り捨てをおこなっています。\n",
    "\n",
    "また、seed=1は乱数のシードを1で固定しますといった意味で、何度か出てきている再現性確保のためです。\n",
    "\n",
    "出力として得られるtrainとtestを確認してみると、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.datasets.sub_dataset.SubDataset at 0x7f2ed45cef98>"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "と表示されるため、「あれ？数値はどこにいったのかな」と迷いますが、train[0]のようにリストの要素番号を指定すると、数値が表示され、リスト形式で保存されていることがわかります。  \n",
    "※このあたりは、なかなかリファレンスがなかったりするため、挙動を確認しながら進めていくことが必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.369e+01, 2.540e+00, 2.000e+01, 1.070e+02, 1.830e+00, 5.600e-01,\n",
       "        5.000e-01, 5.880e+00, 9.600e-01, 6.800e+02], dtype=float32), 2)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.datasets.sub_dataset.SubDataset at 0x7f2ed45ce0b8>"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 12.17,   2.53,  19.  , 104.  ,   1.89,   1.75,   0.45,   2.95,\n",
       "          1.45, 355.  ], dtype=float32), 1)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteratorの設定\n",
    "\n",
    "**Iterator**では「**バッチサイズ**」を決めることができます。\n",
    "\n",
    "順伝播で評価関数を計算する際に、全てのサンプルを使用するのではなく、基本的には、**ミニバッチ**と呼ばれるサンプルの一部のデータセットのみで、評価関数の計算を行い、逆伝播で勾配情報を計算し、最適化アルゴリズム（SGDやAdam等）によりパラメータの学習を行います。\n",
    "\n",
    "### ミニバッチを採用する理由\n",
    "\n",
    "たとえば、10万サンプルある場合は、10万回順伝播を計算して、初めて1回パラメータ更新できるといったように、サンプル数が多ければ多いほど１回あたりのパラメータ更新にかかる時間が長くなってしまうといった問題を避けられます。\n",
    "バッチサイズを10としておけば、ほとんど同じ計算負荷でも1万回のパラメータ更新を行うことができます（厳密には逆伝播が毎回走るため同じ計算負荷ではない）。\n",
    "\n",
    "また、もう一つの理由として、ミニバッチに分けて最適化を行うことで、局所最適解に陥ることを避けられると言われています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はこのバッチサイズを20と設定して行きます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 20\n",
    "train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
    "test_iter  = chainer.iterators.SerialIterator(test,  batchsize, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updaterの設定\n",
    "\n",
    "Updaterでは、Optimizerの設定や、使用するデバイス（CPUやGPU）の設定を行えます。\n",
    "\n",
    "- CPUを使用する場合には、device=-1とオプションに指定しましょう。\n",
    "- GPUを使用する場合には、device=0（GPUを複数枚さしている場合はdevice=1なども存在）とオプションで明示しておきましょう。\n",
    "\n",
    "deviceを特に指定しない場合には、CPUが使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer import training\n",
    "updater = training.StandardUpdater(train_iter, optimizer, device=-1)  # if device=0, compute with gpu\n",
    "# updater = training.StandardUpdater(train_iter, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainerとextensionsの設定\n",
    "Trainerでは、**エポック（ミニバッチを全て処理して１エポック）**の回数や、そのextensionsでオプションを指定することにより、**結果をログ出力や標準出力（インタラクティブに表示）**もできたりします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainerとそのextensionsの設定\n",
    "from chainer.training import extensions\n",
    "\n",
    "# trainerの基本設定\n",
    "epoch = 1000\n",
    "trainer = training.Trainer(updater, (epoch, 'epoch'), out='result')\n",
    "\n",
    "# 検証データで評価\n",
    "trainer.extend(extensions.Evaluator(test_iter, model, device=-1))  # if device=0, compute with gpu\n",
    "\n",
    "# 学習結果の途中を表示する\n",
    "trainer.extend(extensions.LogReport(trigger=(1, 'epoch')))\n",
    "\n",
    "# １エポックごとに、trainデータに対するlossと、testデータに対するloss、経過時間（elapsed_time）を標準出力させる\n",
    "trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'validation/main/loss', 'elapsed_time']), trigger=(1, 'epoch'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の実行\n",
    "色々と設定を行い、Trainerでは最後に trainer.run() でOKです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   validation/main/loss  elapsed_time\n",
      "\u001b[J1           15.7672     14.2523               0.0380148     \n",
      "\u001b[J2           13.3503     12.5375               0.0661485     \n",
      "\u001b[J3           12.0078     11.0668               0.0904122     \n",
      "\u001b[J4           10.174      9.61617               0.113098      \n",
      "\u001b[J5           8.77633     8.20031               0.130908      \n",
      "\u001b[J6           7.47279     6.46364               0.150467      \n",
      "\u001b[J7           5.65272     4.9057                0.167728      \n",
      "\u001b[J8           4.37565     3.70569               0.184991      \n",
      "\u001b[J9           2.96045     2.68607               0.202189      \n",
      "\u001b[J10          2.18111     1.88635               0.219475      \n",
      "\u001b[J11          1.71607     1.49578               0.239101      \n",
      "\u001b[J12          1.42262     1.37366               0.257349      \n",
      "\u001b[J13          1.35401     1.23788               0.274641      \n",
      "\u001b[J14          1.19969     1.13354               0.292101      \n",
      "\u001b[J15          1.12737     1.06224               0.309348      \n",
      "\u001b[J16          1.10141     1.0085                0.328879      \n",
      "\u001b[J17          1.01607     0.975706              0.346177      \n",
      "\u001b[J18          0.943047    0.963266              0.364026      \n",
      "\u001b[J19          0.945658    0.938662              0.38143       \n",
      "\u001b[J20          0.925851    0.922611              0.398817      \n",
      "\u001b[J21          0.906179    0.915853              0.418372      \n",
      "\u001b[J22          0.866938    0.905177              0.435842      \n",
      "\u001b[J23          0.858738    0.889851              0.45431       \n",
      "\u001b[J24          0.837249    0.885791              0.472027      \n",
      "\u001b[J25          0.852619    0.856788              0.4897        \n",
      "\u001b[J26          0.831638    0.870832              0.509421      \n",
      "\u001b[J27          0.782078    0.843597              0.526941      \n",
      "\u001b[J28          0.835115    0.831262              0.544387      \n",
      "\u001b[J29          0.772933    0.856222              0.561944      \n",
      "\u001b[J30          0.810015    0.82852               0.5795        \n",
      "\u001b[J31          0.796734    0.807175              0.599296      \n",
      "\u001b[J32          0.751022    0.820135              0.61695       \n",
      "\u001b[J33          0.72716     0.823868              0.63449       \n",
      "\u001b[J34          0.773825    0.796114              0.652187      \n",
      "\u001b[J35          0.743825    0.798411              0.670975      \n",
      "\u001b[J36          0.736119    0.808466              0.691113      \n",
      "\u001b[J37          0.742908    0.787518              0.708981      \n",
      "\u001b[J38          0.722947    0.7793                0.727221      \n",
      "\u001b[J39          0.706121    0.778957              0.745236      \n",
      "\u001b[J40          0.713209    0.779284              0.76317       \n",
      "\u001b[J41          0.703094    0.779431              0.783429      \n",
      "\u001b[J42          0.6862      0.774346              0.80001       \n",
      "\u001b[J43          0.700637    0.76684               0.816197      \n",
      "\u001b[J44          0.703086    0.760519              0.832545      \n",
      "\u001b[J45          0.691179    0.764719              0.848824      \n",
      "\u001b[J46          0.672736    0.751779              0.866947      \n",
      "\u001b[J47          0.670737    0.754631              0.883379      \n",
      "\u001b[J48          0.708742    0.745453              0.90043       \n",
      "\u001b[J49          0.692149    0.75106               0.916733      \n",
      "\u001b[J50          0.646548    0.742899              0.933235      \n",
      "\u001b[J51          0.642588    0.731351              0.951439      \n",
      "\u001b[J52          0.690679    0.733553              0.96757       \n",
      "\u001b[J53          0.644585    0.731614              0.984117      \n",
      "\u001b[J54          0.673405    0.731231              1.00067       \n",
      "\u001b[J55          0.642413    0.728884              1.01679       \n",
      "\u001b[J56          0.650266    0.726211              1.03475       \n",
      "\u001b[J57          0.647791    0.709584              1.05131       \n",
      "\u001b[J58          0.652518    0.708164              1.06755       \n",
      "\u001b[J59          0.618469    0.73868               1.08398       \n",
      "\u001b[J60          0.647412    0.716446              1.10155       \n",
      "\u001b[J61          0.616736    0.697514              1.11998       \n",
      "\u001b[J62          0.660267    0.71171               1.13612       \n",
      "\u001b[J63          0.626266    0.708043              1.15272       \n",
      "\u001b[J64          0.617325    0.691614              1.1695        \n",
      "\u001b[J65          0.625218    0.694376              1.18625       \n",
      "\u001b[J66          0.626514    0.70803               1.20503       \n",
      "\u001b[J67          0.617019    0.680569              1.22158       \n",
      "\u001b[J68          0.627282    0.691542              1.23825       \n",
      "\u001b[J69          0.603315    0.691641              1.25515       \n",
      "\u001b[J70          0.621237    0.701556              1.27215       \n",
      "\u001b[J71          0.589939    0.681264              1.29103       \n",
      "\u001b[J72          0.614826    0.672677              1.30846       \n",
      "\u001b[J73          0.617224    0.669846              1.32487       \n",
      "\u001b[J74          0.59488     0.677022              1.34131       \n",
      "\u001b[J75          0.586364    0.679059              1.35785       \n",
      "\u001b[J76          0.614234    0.674178              1.37632       \n",
      "\u001b[J77          0.578194    0.672491              1.39282       \n",
      "\u001b[J78          0.587796    0.656024              1.40999       \n",
      "\u001b[J79          0.575776    0.662393              1.42699       \n",
      "\u001b[J80          0.591439    0.669658              1.44417       \n",
      "\u001b[J81          0.596092    0.65945               1.46292       \n",
      "\u001b[J82          0.559855    0.654391              1.47986       \n",
      "\u001b[J83          0.591489    0.658976              1.49662       \n",
      "\u001b[J84          0.567151    0.675008              1.51438       \n",
      "\u001b[J85          0.587039    0.655533              1.53122       \n",
      "\u001b[J86          0.616605    0.642022              1.54987       \n",
      "\u001b[J87          0.527041    0.66892               1.56675       \n",
      "\u001b[J88          0.596032    0.648034              1.58376       \n",
      "\u001b[J89          0.550277    0.642392              1.60752       \n",
      "\u001b[J90          0.573683    0.640233              1.63339       \n",
      "\u001b[J91          0.555815    0.653779              1.66235       \n",
      "\u001b[J92          0.601826    0.629601              1.68827       \n",
      "\u001b[J93          0.539636    0.643764              1.71916       \n",
      "\u001b[J94          0.549519    0.628247              1.75947       \n",
      "\u001b[J95          0.569072    0.641602              1.79126       \n",
      "\u001b[J96          0.5666      0.646587              1.82209       \n",
      "\u001b[J97          0.529927    0.61829               1.847         \n",
      "\u001b[J98          0.560912    0.626019              1.87014       \n",
      "\u001b[J99          0.553097    0.653128              1.89183       \n",
      "\u001b[J100         0.549774    0.617032              1.91247       \n",
      "\u001b[J101         0.576393    0.614639              1.93517       \n",
      "\u001b[J102         0.488216    0.641435              1.9541        \n",
      "\u001b[J103         0.577247    0.62438               1.97311       \n",
      "\u001b[J104         0.551859    0.607589              1.99202       \n",
      "\u001b[J105         0.518474    0.629777              2.01113       \n",
      "\u001b[J106         0.52228     0.617746              2.03242       \n",
      "\u001b[J107         0.544205    0.604582              2.05144       \n",
      "\u001b[J108         0.521928    0.616042              2.07048       \n",
      "\u001b[J109         0.551464    0.607708              2.08956       \n",
      "\u001b[J110         0.508834    0.595694              2.10859       \n",
      "\u001b[J111         0.510939    0.60666               2.12993       \n",
      "\u001b[J112         0.508587    0.614141              2.14988       \n",
      "\u001b[J113         0.55574     0.596117              2.16928       \n",
      "\u001b[J114         0.495811    0.593103              2.1885        \n",
      "\u001b[J115         0.517853    0.604692              2.20768       \n",
      "\u001b[J116         0.503421    0.603426              2.22918       \n",
      "\u001b[J117         0.551887    0.585791              2.24868       \n",
      "\u001b[J118         0.468829    0.598852              2.26826       \n",
      "\u001b[J119         0.528657    0.583555              2.28768       \n",
      "\u001b[J120         0.494581    0.576615              2.30714       \n",
      "\u001b[J121         0.508143    0.601264              2.32877       \n",
      "\u001b[J122         0.49242     0.585682              2.34858       \n",
      "\u001b[J123         0.506475    0.5736                2.37257       \n",
      "\u001b[J124         0.500098    0.569074              2.39237       \n",
      "\u001b[J125         0.496066    0.584481              2.41202       \n",
      "\u001b[J126         0.489008    0.590407              2.43454       \n",
      "\u001b[J127         0.503101    0.568903              2.45494       \n",
      "\u001b[J128         0.498317    0.571041              2.47506       \n",
      "\u001b[J129         0.481129    0.566694              2.49529       \n",
      "\u001b[J130         0.481713    0.569173              2.51553       \n",
      "\u001b[J131         0.487163    0.564509              2.53806       \n",
      "\u001b[J132         0.477827    0.563598              2.55845       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[J133         0.501041    0.549637              2.57937       \n",
      "\u001b[J134         0.452413    0.560775              2.60029       \n",
      "\u001b[J135         0.476788    0.57742               2.62041       \n",
      "\u001b[J136         0.459648    0.558967              2.64297       \n",
      "\u001b[J137         0.495142    0.552991              2.66359       \n",
      "\u001b[J138         0.47645     0.549767              2.68339       \n",
      "\u001b[J139         0.466221    0.546844              2.70202       \n",
      "\u001b[J140         0.453238    0.543169              2.72019       \n",
      "\u001b[J141         0.466437    0.544094              2.74054       \n",
      "\u001b[J142         0.453863    0.540932              2.7589        \n",
      "\u001b[J143         0.478547    0.557772              2.77699       \n",
      "\u001b[J144         0.453366    0.549596              2.7963        \n",
      "\u001b[J145         0.462467    0.528788              2.81462       \n",
      "\u001b[J146         0.495935    0.550652              2.83479       \n",
      "\u001b[J147         0.436897    0.53359               2.85294       \n",
      "\u001b[J148         0.420871    0.522866              2.87087       \n",
      "\u001b[J149         0.468632    0.546052              2.88912       \n",
      "\u001b[J150         0.446807    0.532637              2.90762       \n",
      "\u001b[J151         0.458324    0.519813              2.92804       \n",
      "\u001b[J152         0.402864    0.540237              2.94645       \n",
      "\u001b[J153         0.490004    0.518024              2.96471       \n",
      "\u001b[J154         0.440098    0.52133               2.98295       \n",
      "\u001b[J155         0.416782    0.523079              3.00115       \n",
      "\u001b[J156         0.439018    0.515715              3.02266       \n",
      "\u001b[J157         0.418872    0.516625              3.04125       \n",
      "\u001b[J158         0.454736    0.510913              3.05984       \n",
      "\u001b[J159         0.41835     0.524095              3.07855       \n",
      "\u001b[J160         0.439336    0.513556              3.09682       \n",
      "\u001b[J161         0.413827    0.499636              3.11725       \n",
      "\u001b[J162         0.435234    0.494639              3.13598       \n",
      "\u001b[J163         0.433521    0.528197              3.15535       \n",
      "\u001b[J164         0.435007    0.511408              3.17408       \n",
      "\u001b[J165         0.419       0.495821              3.19275       \n",
      "\u001b[J166         0.435673    0.518355              3.21411       \n",
      "\u001b[J167         0.431335    0.493265              3.23321       \n",
      "\u001b[J168         0.373018    0.480862              3.25149       \n",
      "\u001b[J169         0.426447    0.496831              3.27012       \n",
      "\u001b[J170         0.424587    0.520748              3.28854       \n",
      "\u001b[J171         0.438301    0.479327              3.30931       \n",
      "\u001b[J172         0.385414    0.484087              3.32806       \n",
      "\u001b[J173         0.375784    0.498683              3.34706       \n",
      "\u001b[J174         0.412452    0.490856              3.36563       \n",
      "\u001b[J175         0.413212    0.471489              3.38499       \n",
      "\u001b[J176         0.398644    0.495431              3.40585       \n",
      "\u001b[J177         0.421811    0.480675              3.42565       \n",
      "\u001b[J178         0.36499     0.474497              3.44518       \n",
      "\u001b[J179         0.421167    0.472745              3.46465       \n",
      "\u001b[J180         0.38177     0.473658              3.48377       \n",
      "\u001b[J181         0.38591     0.48096               3.50465       \n",
      "\u001b[J182         0.406395    0.464622              3.52355       \n",
      "\u001b[J183         0.369757    0.457007              3.54272       \n",
      "\u001b[J184         0.385729    0.469662              3.56192       \n",
      "\u001b[J185         0.386913    0.467863              3.58083       \n",
      "\u001b[J186         0.383       0.457997              3.60191       \n",
      "\u001b[J187         0.396057    0.46157               3.62104       \n",
      "\u001b[J188         0.401253    0.464697              3.64085       \n",
      "\u001b[J189         0.341207    0.454353              3.66049       \n",
      "\u001b[J190         0.382901    0.450244              3.67952       \n",
      "\u001b[J191         0.394544    0.44501               3.70052       \n",
      "\u001b[J192         0.376236    0.469488              3.71967       \n",
      "\u001b[J193         0.358491    0.450558              3.73872       \n",
      "\u001b[J194         0.378377    0.452595              3.75792       \n",
      "\u001b[J195         0.369576    0.462638              3.77706       \n",
      "\u001b[J196         0.359686    0.430483              3.79821       \n",
      "\u001b[J197         0.38512     0.446078              3.81989       \n",
      "\u001b[J198         0.368772    0.456531              3.83912       \n",
      "\u001b[J199         0.363822    0.430643              3.85968       \n",
      "\u001b[J200         0.366137    0.426616              3.87896       \n",
      "\u001b[J201         0.35502     0.429432              3.90002       \n",
      "\u001b[J202         0.368464    0.438965              3.91916       \n",
      "\u001b[J203         0.363738    0.446877              3.93886       \n",
      "\u001b[J204         0.375587    0.428853              3.95831       \n",
      "\u001b[J205         0.338957    0.433197              3.97797       \n",
      "\u001b[J206         0.357281    0.41585               3.99918       \n",
      "\u001b[J207         0.397092    0.448016              4.01844       \n",
      "\u001b[J208         0.291328    0.423771              4.03773       \n",
      "\u001b[J209         0.366893    0.418894              4.05731       \n",
      "\u001b[J210         0.34779     0.412875              4.07787       \n",
      "\u001b[J211         0.324196    0.442755              4.09956       \n",
      "\u001b[J212         0.368536    0.416003              4.11902       \n",
      "\u001b[J213         0.348679    0.4128                4.13882       \n",
      "\u001b[J214         0.333632    0.423161              4.15855       \n",
      "\u001b[J215         0.342228    0.413126              4.17806       \n",
      "\u001b[J216         0.339761    0.416571              4.19965       \n",
      "\u001b[J217         0.349267    0.413049              4.2192        \n",
      "\u001b[J218         0.322081    0.406516              4.23896       \n",
      "\u001b[J219         0.329394    0.414387              4.25868       \n",
      "\u001b[J220         0.351673    0.410943              4.27831       \n",
      "\u001b[J221         0.327306    0.420124              4.30063       \n",
      "\u001b[J222         0.325974    0.394888              4.32041       \n",
      "\u001b[J223         0.337033    0.399907              4.33998       \n",
      "\u001b[J224         0.334118    0.422343              4.35973       \n",
      "\u001b[J225         0.331047    0.405215              4.37941       \n",
      "\u001b[J226         0.332051    0.390239              4.401         \n",
      "\u001b[J227         0.33377     0.401605              4.42053       \n",
      "\u001b[J228         0.31761     0.393804              4.4405        \n",
      "\u001b[J229         0.309977    0.401098              4.46042       \n",
      "\u001b[J230         0.328371    0.389832              4.48013       \n",
      "\u001b[J231         0.320131    0.387969              4.50188       \n",
      "\u001b[J232         0.30371     0.402542              4.52262       \n",
      "\u001b[J233         0.325713    0.38282               4.54251       \n",
      "\u001b[J234         0.331919    0.387318              4.56224       \n",
      "\u001b[J235         0.304323    0.396733              4.58222       \n",
      "\u001b[J236         0.307009    0.386614              4.60397       \n",
      "\u001b[J237         0.305462    0.377109              4.62388       \n",
      "\u001b[J238         0.33341     0.376344              4.64355       \n",
      "\u001b[J239         0.3033      0.38489               4.66339       \n",
      "\u001b[J240         0.311289    0.380216              4.6839        \n",
      "\u001b[J241         0.293258    0.37884               4.70607       \n",
      "\u001b[J242         0.342776    0.382961              4.72657       \n",
      "\u001b[J243         0.304284    0.399127              4.74957       \n",
      "\u001b[J244         0.299244    0.358743              4.76982       \n",
      "\u001b[J245         0.313966    0.37079               4.79061       \n",
      "\u001b[J246         0.304481    0.381231              4.81283       \n",
      "\u001b[J247         0.307172    0.367567              4.83291       \n",
      "\u001b[J248         0.283651    0.38007               4.85305       \n",
      "\u001b[J249         0.308216    0.366083              4.87323       \n",
      "\u001b[J250         0.301708    0.35979               4.8934        \n",
      "\u001b[J251         0.318256    0.373368              4.9155        \n",
      "\u001b[J252         0.299289    0.355102              4.93587       \n",
      "\u001b[J253         0.299966    0.385032              4.95705       \n",
      "\u001b[J254         0.31892     0.363715              4.97727       \n",
      "\u001b[J255         0.287567    0.36167               4.99761       \n",
      "\u001b[J256         0.31746     0.380988              5.02053       \n",
      "\u001b[J257         0.293394    0.343583              5.04141       \n",
      "\u001b[J258         0.298275    0.38686               5.06172       \n",
      "\u001b[J259         0.284709    0.362817              5.08267       \n",
      "\u001b[J260         0.293045    0.351582              5.1032        \n",
      "\u001b[J261         0.282817    0.357469              5.12564       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[J262         0.292035    0.355452              5.14689       \n",
      "\u001b[J263         0.275535    0.345901              5.16827       \n",
      "\u001b[J264         0.289244    0.354271              5.18888       \n",
      "\u001b[J265         0.291669    0.349343              5.20951       \n",
      "\u001b[J266         0.286734    0.344604              5.23222       \n",
      "\u001b[J267         0.296139    0.375738              5.25305       \n",
      "\u001b[J268         0.27727     0.340433              5.27375       \n",
      "\u001b[J269         0.28017     0.34408               5.29466       \n",
      "\u001b[J270         0.280624    0.352704              5.31544       \n",
      "\u001b[J271         0.29958     0.34138               5.3383        \n",
      "\u001b[J272         0.265355    0.348621              5.35976       \n",
      "\u001b[J273         0.251455    0.347427              5.38232       \n",
      "\u001b[J274         0.286019    0.349018              5.40317       \n",
      "\u001b[J275         0.285064    0.333402              5.42431       \n",
      "\u001b[J276         0.281786    0.356335              5.44718       \n",
      "\u001b[J277         0.256942    0.347424              5.4682        \n",
      "\u001b[J278         0.282881    0.321016              5.48924       \n",
      "\u001b[J279         0.287412    0.36106               5.51015       \n",
      "\u001b[J280         0.271001    0.334178              5.5336        \n",
      "\u001b[J281         0.282199    0.32152               5.55661       \n",
      "\u001b[J282         0.251164    0.327614              5.57835       \n",
      "\u001b[J283         0.266823    0.328102              5.60009       \n",
      "\u001b[J284         0.26775     0.339522              5.62154       \n",
      "\u001b[J285         0.271656    0.323621              5.64276       \n",
      "\u001b[J286         0.256578    0.329737              5.66596       \n",
      "\u001b[J287         0.271735    0.340108              5.68711       \n",
      "\u001b[J288         0.266488    0.322812              5.70817       \n",
      "\u001b[J289         0.259348    0.320094              5.72912       \n",
      "\u001b[J290         0.270396    0.328263              5.75004       \n",
      "\u001b[J291         0.258895    0.322638              5.77318       \n",
      "\u001b[J292         0.258122    0.344426              5.79535       \n",
      "\u001b[J293         0.275746    0.324087              5.81666       \n",
      "\u001b[J294         0.246184    0.31825               5.838         \n",
      "\u001b[J295         0.260286    0.308677              5.85946       \n",
      "\u001b[J296         0.265393    0.31726               5.88295       \n",
      "\u001b[J297         0.241362    0.329383              5.90469       \n",
      "\u001b[J298         0.255088    0.31173               5.92642       \n",
      "\u001b[J299         0.276071    0.329865              5.94822       \n",
      "\u001b[J300         0.257054    0.324575              5.96969       \n",
      "\u001b[J301         0.266352    0.32069               5.99331       \n",
      "\u001b[J302         0.280618    0.316304              6.01547       \n",
      "\u001b[J303         0.235067    0.29685               6.03804       \n",
      "\u001b[J304         0.267568    0.313751              6.05967       \n",
      "\u001b[J305         0.247849    0.366187              6.08127       \n",
      "\u001b[J306         0.275081    0.302198              6.1048        \n",
      "\u001b[J307         0.237396    0.312898              6.12643       \n",
      "\u001b[J308         0.249253    0.303309              6.14828       \n",
      "\u001b[J309         0.234652    0.302158              6.17033       \n",
      "\u001b[J310         0.249374    0.322354              6.19185       \n",
      "\u001b[J311         0.240941    0.313066              6.21553       \n",
      "\u001b[J312         0.248661    0.306441              6.23817       \n",
      "\u001b[J313         0.250117    0.295101              6.26017       \n",
      "\u001b[J314         0.240233    0.302006              6.28192       \n",
      "\u001b[J315         0.253467    0.320684              6.30341       \n",
      "\u001b[J316         0.227645    0.309027              6.32703       \n",
      "\u001b[J317         0.265715    0.305463              6.34876       \n",
      "\u001b[J318         0.233636    0.294903              6.3705        \n",
      "\u001b[J319         0.237664    0.305265              6.39218       \n",
      "\u001b[J320         0.246099    0.31427               6.41401       \n",
      "\u001b[J321         0.259953    0.297956              6.43782       \n",
      "\u001b[J322         0.226046    0.282585              6.4597        \n",
      "\u001b[J323         0.225045    0.29744               6.48312       \n",
      "\u001b[J324         0.240209    0.309028              6.50567       \n",
      "\u001b[J325         0.246721    0.294214              6.52782       \n",
      "\u001b[J326         0.236214    0.307387              6.69783       \n",
      "\u001b[J327         0.233732    0.28781               6.7348        \n",
      "\u001b[J328         0.234972    0.288172              6.76661       \n",
      "\u001b[J329         0.244626    0.297124              6.79516       \n",
      "\u001b[J330         0.228705    0.305104              6.82187       \n",
      "\u001b[J331         0.234193    0.290518              6.84785       \n",
      "\u001b[J332         0.226027    0.292374              6.87007       \n",
      "\u001b[J333         0.236416    0.280767              6.89219       \n",
      "\u001b[J334         0.231605    0.29455               6.91491       \n",
      "\u001b[J335         0.2371      0.301874              6.93767       \n",
      "\u001b[J336         0.212257    0.290251              6.96213       \n",
      "\u001b[J337         0.264117    0.293912              6.9844        \n",
      "\u001b[J338         0.215369    0.295048              7.00663       \n",
      "\u001b[J339         0.231757    0.296546              7.02877       \n",
      "\u001b[J340         0.228552    0.281754              7.05093       \n",
      "\u001b[J341         0.225485    0.285083              7.0752        \n",
      "\u001b[J342         0.233705    0.279427              7.09753       \n",
      "\u001b[J343         0.218231    0.297276              7.11985       \n",
      "\u001b[J344         0.235201    0.299268              7.14352       \n",
      "\u001b[J345         0.231026    0.267872              7.1662        \n",
      "\u001b[J346         0.21911     0.291748              7.19031       \n",
      "\u001b[J347         0.241015    0.302502              7.21284       \n",
      "\u001b[J348         0.214712    0.271311              7.23513       \n",
      "\u001b[J349         0.24142     0.277626              7.25749       \n",
      "\u001b[J350         0.221032    0.289012              7.27987       \n",
      "\u001b[J351         0.267673    0.280226              7.30421       \n",
      "\u001b[J352         0.162757    0.284835              7.32638       \n",
      "\u001b[J353         0.224687    0.279167              7.34976       \n",
      "\u001b[J354         0.246087    0.282942              7.37233       \n",
      "\u001b[J355         0.207424    0.270969              7.39486       \n",
      "\u001b[J356         0.240007    0.285128              7.41944       \n",
      "\u001b[J357         0.196085    0.291304              7.44222       \n",
      "\u001b[J358         0.218594    0.28028               7.46473       \n",
      "\u001b[J359         0.225116    0.263769              7.48728       \n",
      "\u001b[J360         0.220147    0.285674              7.51008       \n",
      "\u001b[J361         0.224147    0.272172              7.53426       \n",
      "\u001b[J362         0.19803     0.27966               7.55815       \n",
      "\u001b[J363         0.218848    0.279417              7.58112       \n",
      "\u001b[J364         0.221866    0.260744              7.60387       \n",
      "\u001b[J365         0.218713    0.271535              7.62703       \n",
      "\u001b[J366         0.208467    0.281666              7.65183       \n",
      "\u001b[J367         0.219987    0.280333              7.67497       \n",
      "\u001b[J368         0.208901    0.267368              7.69828       \n",
      "\u001b[J369         0.229474    0.267472              7.72124       \n",
      "\u001b[J370         0.203811    0.259973              7.74464       \n",
      "\u001b[J371         0.21173     0.274153              7.77104       \n",
      "\u001b[J372         0.204256    0.288347              7.7939        \n",
      "\u001b[J373         0.221526    0.268622              7.81705       \n",
      "\u001b[J374         0.205617    0.263241              7.84004       \n",
      "\u001b[J375         0.213855    0.259546              7.86296       \n",
      "\u001b[J376         0.236576    0.274457              7.88795       \n",
      "\u001b[J377         0.181113    0.264916              7.91125       \n",
      "\u001b[J378         0.208253    0.266147              7.934         \n",
      "\u001b[J379         0.216201    0.293483              7.95726       \n",
      "\u001b[J380         0.219123    0.252879              7.98149       \n",
      "\u001b[J381         0.200095    0.281477              8.00675       \n",
      "\u001b[J382         0.221556    0.261615              8.02959       \n",
      "\u001b[J383         0.210371    0.28451               8.05475       \n",
      "\u001b[J384         0.205352    0.270257              8.0787        \n",
      "\u001b[J385         0.210777    0.257561              8.10187       \n",
      "\u001b[J386         0.202188    0.254426              8.12681       \n",
      "\u001b[J387         0.200691    0.271721              8.14986       \n",
      "\u001b[J388         0.219962    0.26262               8.1738        \n",
      "\u001b[J389         0.205825    0.25115               8.19853       \n",
      "\u001b[J390         0.186607    0.258145              8.22173       \n",
      "\u001b[J391         0.184955    0.25919               8.24724       \n",
      "\u001b[J392         0.211982    0.261205              8.27055       \n",
      "\u001b[J393         0.213651    0.270069              8.29414       \n",
      "\u001b[J394         0.218271    0.253798              8.3176        \n",
      "\u001b[J395         0.202618    0.243478              8.34107       \n",
      "\u001b[J396         0.187163    0.272486              8.36651       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[J397         0.211315    0.255492              8.39445       \n",
      "\u001b[J398         0.197162    0.262523              8.41843       \n",
      "\u001b[J399         0.209669    0.271295              8.44205       \n",
      "\u001b[J400         0.203969    0.24593               8.46595       \n",
      "\u001b[J401         0.188801    0.282057              8.49148       \n",
      "\u001b[J402         0.204638    0.253193              8.51513       \n",
      "\u001b[J403         0.207561    0.251404              8.53871       \n",
      "\u001b[J404         0.188855    0.249149              8.56233       \n",
      "\u001b[J405         0.199377    0.261278              8.58588       \n",
      "\u001b[J406         0.204509    0.266341              8.6122        \n",
      "\u001b[J407         0.178978    0.235112              8.63639       \n",
      "\u001b[J408         0.197585    0.245864              8.66029       \n",
      "\u001b[J409         0.201479    0.265315              8.68401       \n",
      "\u001b[J410         0.192346    0.256824              8.70757       \n",
      "\u001b[J411         0.189591    0.274905              8.7332        \n",
      "\u001b[J412         0.186871    0.241233              8.7571        \n",
      "\u001b[J413         0.203923    0.249144              8.78077       \n",
      "\u001b[J414         0.197907    0.269648              8.80441       \n",
      "\u001b[J415         0.206384    0.235739              8.82931       \n",
      "\u001b[J416         0.196257    0.272834              8.85556       \n",
      "\u001b[J417         0.186111    0.242634              8.87933       \n",
      "\u001b[J418         0.197527    0.249405              8.90329       \n",
      "\u001b[J419         0.187723    0.265485              8.92747       \n",
      "\u001b[J420         0.191651    0.258141              8.95208       \n",
      "\u001b[J421         0.181423    0.233506              8.97839       \n",
      "\u001b[J422         0.18184     0.237864              9.0025        \n",
      "\u001b[J423         0.20759     0.25824               9.02682       \n",
      "\u001b[J424         0.188459    0.270502              9.05171       \n",
      "\u001b[J425         0.194089    0.238649              9.0761        \n",
      "\u001b[J426         0.174289    0.236303              9.10224       \n",
      "\u001b[J427         0.192545    0.261223              9.12662       \n",
      "\u001b[J428         0.197349    0.241519              9.15833       \n",
      "\u001b[J429         0.206949    0.239263              9.18242       \n",
      "\u001b[J430         0.211764    0.275525              9.20671       \n",
      "\u001b[J431         0.215159    0.252556              9.2328        \n",
      "\u001b[J432         0.155853    0.261586              9.2569        \n",
      "\u001b[J433         0.182906    0.231365              9.28218       \n",
      "\u001b[J434         0.207902    0.234157              9.30673       \n",
      "\u001b[J435         0.180148    0.276776              9.33128       \n",
      "\u001b[J436         0.181362    0.2378                9.35797       \n",
      "\u001b[J437         0.199468    0.22872               9.38226       \n",
      "\u001b[J438         0.185824    0.22976               9.40674       \n",
      "\u001b[J439         0.174341    0.248285              9.43109       \n",
      "\u001b[J440         0.1755      0.256929              9.4555        \n",
      "\u001b[J441         0.182659    0.255347              9.48294       \n",
      "\u001b[J442         0.203197    0.240874              9.50801       \n",
      "\u001b[J443         0.164776    0.236297              9.53277       \n",
      "\u001b[J444         0.185183    0.239875              9.55761       \n",
      "\u001b[J445         0.185575    0.218554              9.58259       \n",
      "\u001b[J446         0.174319    0.24972               9.60926       \n",
      "\u001b[J447         0.19061     0.25294               9.63367       \n",
      "\u001b[J448         0.172913    0.245736              9.65823       \n",
      "\u001b[J449         0.182246    0.231174              9.68272       \n",
      "\u001b[J450         0.182934    0.234421              9.70807       \n",
      "\u001b[J451         0.168193    0.254925              9.84047       \n",
      "\u001b[J452         0.18085     0.240712              10.1599       \n",
      "\u001b[J453         0.192271    0.222569              10.3145       \n",
      "\u001b[J454         0.166773    0.247219              10.6504       \n",
      "\u001b[J455         0.182334    0.246722              10.7364       \n",
      "\u001b[J456         0.164506    0.241221              10.7817       \n",
      "\u001b[J457         0.217982    0.233061              10.8182       \n",
      "\u001b[J458         0.177278    0.233428              10.8516       \n",
      "\u001b[J459         0.147808    0.240461              10.8813       \n",
      "\u001b[J460         0.173022    0.235193              10.9106       \n",
      "\u001b[J461         0.184134    0.235316              10.9422       \n",
      "\u001b[J462         0.169659    0.230663              10.9717       \n",
      "\u001b[J463         0.177591    0.222627              11.0013       \n",
      "\u001b[J464         0.169216    0.25203               11.0305       \n",
      "\u001b[J465         0.177223    0.257867              11.061        \n",
      "\u001b[J466         0.168244    0.214187              11.0953       \n",
      "\u001b[J467         0.188875    0.226968              11.126        \n",
      "\u001b[J468         0.164689    0.253412              11.1555       \n",
      "\u001b[J469         0.175029    0.239243              11.1851       \n",
      "\u001b[J470         0.175582    0.22621               11.2146       \n",
      "\u001b[J471         0.192022    0.230625              11.2466       \n",
      "\u001b[J472         0.129708    0.231772              11.2774       \n",
      "\u001b[J473         0.195712    0.215792              11.3072       \n",
      "\u001b[J474         0.159929    0.23952               11.3368       \n",
      "\u001b[J475         0.165227    0.235845              11.3665       \n",
      "\u001b[J476         0.16863     0.229652              11.3986       \n",
      "\u001b[J477         0.206113    0.230162              11.4283       \n",
      "\u001b[J478         0.13072     0.235044              11.4581       \n",
      "\u001b[J479         0.179023    0.230164              11.4889       \n",
      "\u001b[J480         0.163873    0.238625              11.5186       \n",
      "\u001b[J481         0.16144     0.224333              11.5508       \n",
      "\u001b[J482         0.167659    0.231062              11.5808       \n",
      "\u001b[J483         0.177118    0.232476              11.6078       \n",
      "\u001b[J484         0.163036    0.242504              11.6339       \n",
      "\u001b[J485         0.170238    0.229804              11.6602       \n",
      "\u001b[J486         0.181368    0.223593              11.6894       \n",
      "\u001b[J487         0.144464    0.219653              11.7148       \n",
      "\u001b[J488         0.184249    0.230098              11.7406       \n",
      "\u001b[J489         0.140666    0.236579              11.7661       \n",
      "\u001b[J490         0.168419    0.230859              11.792        \n",
      "\u001b[J491         0.163384    0.225769              11.8194       \n",
      "\u001b[J492         0.15059     0.223323              11.845        \n",
      "\u001b[J493         0.181916    0.22387               11.8706       \n",
      "\u001b[J494         0.160323    0.219625              11.8963       \n",
      "\u001b[J495         0.155968    0.213019              11.943        \n",
      "\u001b[J496         0.156117    0.246445              11.9985       \n",
      "\u001b[J497         0.165235    0.239182              12.0399       \n",
      "\u001b[J498         0.171457    0.213569              12.0763       \n",
      "\u001b[J499         0.1583      0.222379              12.1095       \n",
      "\u001b[J500         0.165214    0.234654              12.1398       \n",
      "\u001b[J501         0.173817    0.21082               12.1717       \n",
      "\u001b[J502         0.14878     0.211656              12.2009       \n",
      "\u001b[J503         0.15606     0.244544              12.23         \n",
      "\u001b[J504         0.168516    0.249653              12.2593       \n",
      "\u001b[J505         0.152217    0.211014              12.2887       \n",
      "\u001b[J506         0.151616    0.210773              12.3203       \n",
      "\u001b[J507         0.169704    0.232275              12.3508       \n",
      "\u001b[J508         0.151522    0.224862              12.3804       \n",
      "\u001b[J509         0.175072    0.231325              12.4099       \n",
      "\u001b[J510         0.148514    0.216168              12.4393       \n",
      "\u001b[J511         0.142685    0.20906               12.4711       \n",
      "\u001b[J512         0.162187    0.226221              12.5009       \n",
      "\u001b[J513         0.168295    0.223889              12.53         \n",
      "\u001b[J514         0.160064    0.223824              12.5747       \n",
      "\u001b[J515         0.155841    0.203919              12.6224       \n",
      "\u001b[J516         0.155524    0.227446              12.6654       \n",
      "\u001b[J517         0.162192    0.256447              12.7005       \n",
      "\u001b[J518         0.157976    0.209688              12.732        \n",
      "\u001b[J519         0.156079    0.213985              12.7625       \n",
      "\u001b[J520         0.155327    0.213821              12.792        \n",
      "\u001b[J521         0.151868    0.22712               12.8238       \n",
      "\u001b[J522         0.172366    0.215359              12.8537       \n",
      "\u001b[J523         0.145759    0.228197              12.8836       \n",
      "\u001b[J524         0.154594    0.218241              12.9136       \n",
      "\u001b[J525         0.153584    0.212399              12.9436       \n",
      "\u001b[J526         0.160136    0.239436              12.9769       \n",
      "\u001b[J527         0.139929    0.243962              13.007        \n",
      "\u001b[J528         0.17139     0.192321              13.037        \n",
      "\u001b[J529         0.144108    0.227638              13.067        \n",
      "\u001b[J530         0.162087    0.224066              13.0971       \n",
      "\u001b[J531         0.154481    0.214766              13.1292       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[J532         0.148638    0.225575              13.1592       \n",
      "\u001b[J533         0.141184    0.226849              13.1903       \n",
      "\u001b[J534         0.154841    0.206759              13.2205       \n",
      "\u001b[J535         0.14784     0.202188              13.2506       \n",
      "\u001b[J536         0.136321    0.217483              13.283        \n",
      "\u001b[J537         0.165096    0.224857              13.3132       \n",
      "\u001b[J538         0.147513    0.244813              13.3435       \n",
      "\u001b[J539         0.154911    0.21855               13.3737       \n",
      "\u001b[J540         0.172646    0.199516              13.4054       \n",
      "\u001b[J541         0.155081    0.266788              13.4379       \n",
      "\u001b[J542         0.14236     0.206288              13.4668       \n",
      "\u001b[J543         0.166711    0.202844              13.4933       \n",
      "\u001b[J544         0.135278    0.23961               13.5202       \n",
      "\u001b[J545         0.156363    0.217334              13.5469       \n",
      "\u001b[J546         0.13773     0.213343              13.5757       \n",
      "\u001b[J547         0.162629    0.224793              13.6035       \n",
      "\u001b[J548         0.137834    0.202768              13.6305       \n",
      "\u001b[J549         0.152838    0.216274              13.6575       \n",
      "\u001b[J550         0.145231    0.205706              13.6845       \n",
      "\u001b[J551         0.143841    0.231592              13.7134       \n",
      "\u001b[J552         0.177344    0.230746              13.7402       \n",
      "\u001b[J553         0.139248    0.176972              13.7673       \n",
      "\u001b[J554         0.150924    0.199057              13.7942       \n",
      "\u001b[J555         0.147664    0.267243              13.8217       \n",
      "\u001b[J556         0.152257    0.212413              13.8512       \n",
      "\u001b[J557         0.143604    0.216819              13.878        \n",
      "\u001b[J558         0.134814    0.204239              13.9047       \n",
      "\u001b[J559         0.143374    0.198281              13.9317       \n",
      "\u001b[J560         0.141237    0.211052              13.9585       \n",
      "\u001b[J561         0.141455    0.225124              13.9873       \n",
      "\u001b[J562         0.137044    0.215198              14.0143       \n",
      "\u001b[J563         0.143374    0.215193              14.0411       \n",
      "\u001b[J564         0.147107    0.199922              14.0691       \n",
      "\u001b[J565         0.143057    0.203597              14.0961       \n",
      "\u001b[J566         0.163417    0.246647              14.1252       \n",
      "\u001b[J567         0.117002    0.20209               14.1522       \n",
      "\u001b[J568         0.145841    0.198286              14.1795       \n",
      "\u001b[J569         0.138427    0.23445               14.2069       \n",
      "\u001b[J570         0.150942    0.214                 14.234        \n",
      "\u001b[J571         0.151544    0.206631              14.264        \n",
      "\u001b[J572         0.152613    0.178166              14.2913       \n",
      "\u001b[J573         0.16377     0.225131              14.3185       \n",
      "\u001b[J574         0.108199    0.233716              14.3459       \n",
      "\u001b[J575         0.141865    0.218825              14.373        \n",
      "\u001b[J576         0.13705     0.192212              14.4061       \n",
      "\u001b[J577         0.1297      0.200248              14.4335       \n",
      "\u001b[J578         0.138312    0.198502              14.461        \n",
      "\u001b[J579         0.141825    0.217664              14.4893       \n",
      "\u001b[J580         0.138956    0.214658              14.5174       \n",
      "\u001b[J581         0.127249    0.201159              14.5467       \n",
      "\u001b[J582         0.14434     0.201107              14.5741       \n",
      "\u001b[J583         0.136901    0.208274              14.6014       \n",
      "\u001b[J584         0.13425     0.219101              14.6289       \n",
      "\u001b[J585         0.145994    0.232102              14.6568       \n",
      "\u001b[J586         0.137171    0.181714              14.6864       \n",
      "\u001b[J587         0.133856    0.197946              14.7148       \n",
      "\u001b[J588         0.143301    0.244453              14.7428       \n",
      "\u001b[J589         0.147126    0.220065              14.7705       \n",
      "\u001b[J590         0.13466     0.186782              14.7982       \n",
      "\u001b[J591         0.156663    0.187529              14.828        \n",
      "\u001b[J592         0.109521    0.233829              14.8557       \n",
      "\u001b[J593         0.171403    0.234524              14.8834       \n",
      "\u001b[J594         0.108476    0.201012              14.9111       \n",
      "\u001b[J595         0.144146    0.174109              14.9397       \n",
      "\u001b[J596         0.125294    0.2275                14.9697       \n",
      "\u001b[J597         0.131938    0.228935              14.9972       \n",
      "\u001b[J598         0.142095    0.195233              15.025        \n",
      "\u001b[J599         0.138638    0.199227              15.0528       \n",
      "\u001b[J600         0.139483    0.214895              15.099        \n",
      "\u001b[J601         0.122545    0.207077              15.1597       \n",
      "\u001b[J602         0.144094    0.195764              15.2012       \n",
      "\u001b[J603         0.140678    0.221786              15.2367       \n",
      "\u001b[J604         0.12493     0.190369              15.2691       \n",
      "\u001b[J605         0.128821    0.202167              15.3013       \n",
      "\u001b[J606         0.120323    0.221168              15.3352       \n",
      "\u001b[J607         0.139906    0.198411              15.367        \n",
      "\u001b[J608         0.140095    0.179942              15.4005       \n",
      "\u001b[J609         0.12243     0.197233              15.4324       \n",
      "\u001b[J610         0.136098    0.238882              15.4645       \n",
      "\u001b[J611         0.152522    0.197919              15.4986       \n",
      "\u001b[J612         0.111863    0.185419              15.5311       \n",
      "\u001b[J613         0.137076    0.210443              15.5631       \n",
      "\u001b[J614         0.135995    0.195092              15.5975       \n",
      "\u001b[J615         0.130074    0.243657              15.63         \n",
      "\u001b[J616         0.121323    0.196429              15.6645       \n",
      "\u001b[J617         0.139091    0.209114              15.6976       \n",
      "\u001b[J618         0.116505    0.200599              15.73         \n",
      "\u001b[J619         0.133709    0.207643              15.7623       \n",
      "\u001b[J620         0.131401    0.190228              15.7945       \n",
      "\u001b[J621         0.127754    0.227423              15.8313       \n",
      "\u001b[J622         0.141025    0.197907              15.8637       \n",
      "\u001b[J623         0.104282    0.182052              15.8959       \n",
      "\u001b[J624         0.129663    0.207475              15.9283       \n",
      "\u001b[J625         0.126047    0.209698              15.9589       \n",
      "\u001b[J626         0.127341    0.191584              15.9892       \n",
      "\u001b[J627         0.122834    0.187427              16.0174       \n",
      "\u001b[J628         0.135127    0.23276               16.0456       \n",
      "\u001b[J629         0.116247    0.211375              16.0762       \n",
      "\u001b[J630         0.129122    0.196509              16.1049       \n",
      "\u001b[J631         0.120726    0.197063              16.1351       \n",
      "\u001b[J632         0.121697    0.207617              16.1638       \n",
      "\u001b[J633         0.124398    0.191519              16.1925       \n",
      "\u001b[J634         0.122302    0.205701              16.2235       \n",
      "\u001b[J635         0.124378    0.210425              16.2524       \n",
      "\u001b[J636         0.12066     0.211221              16.2854       \n",
      "\u001b[J637         0.125007    0.225454              16.3141       \n",
      "\u001b[J638         0.124155    0.189892              16.3428       \n",
      "\u001b[J639         0.125527    0.204006              16.3714       \n",
      "\u001b[J640         0.121586    0.212516              16.4001       \n",
      "\u001b[J641         0.128615    0.188681              16.4308       \n",
      "\u001b[J642         0.134307    0.208283              16.4598       \n",
      "\u001b[J643         0.121185    0.201791              16.4903       \n",
      "\u001b[J644         0.101854    0.19764               16.5192       \n",
      "\u001b[J645         0.116875    0.215951              16.5481       \n",
      "\u001b[J646         0.115649    0.202208              16.5789       \n",
      "\u001b[J647         0.143721    0.183307              16.6086       \n",
      "\u001b[J648         0.0941086   0.202221              16.6375       \n",
      "\u001b[J649         0.129479    0.230566              16.6666       \n",
      "\u001b[J650         0.120422    0.189999              16.6981       \n",
      "\u001b[J651         0.10969     0.18664               16.7294       \n",
      "\u001b[J652         0.116351    0.193086              16.7588       \n",
      "\u001b[J653         0.133944    0.226354              16.788        \n",
      "\u001b[J654         0.114916    0.204813              16.8172       \n",
      "\u001b[J655         0.144888    0.160229              16.8467       \n",
      "\u001b[J656         0.13334     0.235489              16.8779       \n",
      "\u001b[J657         0.0929251   0.228618              16.9073       \n",
      "\u001b[J658         0.11966     0.190649              16.9386       \n",
      "\u001b[J659         0.12317     0.186591              16.9677       \n",
      "\u001b[J660         0.115185    0.214068              16.9967       \n",
      "\u001b[J661         0.104463    0.19548               17.028        \n",
      "\u001b[J662         0.115644    0.201423              17.0572       \n",
      "\u001b[J663         0.118157    0.202164              17.0866       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[J664         0.11776     0.208779              17.1161       \n",
      "\u001b[J665         0.124119    0.180235              17.1484       \n",
      "\u001b[J666         0.105005    0.205453              17.1799       \n",
      "\u001b[J667         0.127628    0.209299              17.2099       \n",
      "\u001b[J668         0.109859    0.218079              17.2393       \n",
      "\u001b[J669         0.129974    0.18757               17.2687       \n",
      "\u001b[J670         0.114546    0.198166              17.2983       \n",
      "\u001b[J671         0.105608    0.250774              17.3298       \n",
      "\u001b[J672         0.133081    0.206475              17.3621       \n",
      "\u001b[J673         0.120293    0.208254              17.3925       \n",
      "\u001b[J674         0.114064    0.206064              17.4225       \n",
      "\u001b[J675         0.118904    0.175151              17.4527       \n",
      "\u001b[J676         0.122105    0.190459              17.4843       \n",
      "\u001b[J677         0.110835    0.203931              17.5139       \n",
      "\u001b[J678         0.0959314   0.208111              17.5443       \n",
      "\u001b[J679         0.116629    0.215689              17.577        \n",
      "\u001b[J680         0.112081    0.181199              17.6072       \n",
      "\u001b[J681         0.10644     0.185179              17.6391       \n",
      "\u001b[J682         0.111273    0.212246              17.6689       \n",
      "\u001b[J683         0.128171    0.222711              17.6993       \n",
      "\u001b[J684         0.103104    0.163809              17.7292       \n",
      "\u001b[J685         0.117949    0.192816              17.759        \n",
      "\u001b[J686         0.104053    0.217001              17.7938       \n",
      "\u001b[J687         0.112133    0.202962              17.8236       \n",
      "\u001b[J688         0.112264    0.18746               17.8534       \n",
      "\u001b[J689         0.131884    0.199459              17.8833       \n",
      "\u001b[J690         0.0953036   0.187516              17.9133       \n",
      "\u001b[J691         0.117437    0.219237              17.9451       \n",
      "\u001b[J692         0.0956145   0.201851              17.9749       \n",
      "\u001b[J693         0.110964    0.191561              18.0067       \n",
      "\u001b[J694         0.115607    0.177833              18.0365       \n",
      "\u001b[J695         0.111045    0.213155              18.0665       \n",
      "\u001b[J696         0.116555    0.207786              18.0982       \n",
      "\u001b[J697         0.107632    0.189454              18.1281       \n",
      "\u001b[J698         0.117795    0.217823              18.1583       \n",
      "\u001b[J699         0.105315    0.199253              18.1883       \n",
      "\u001b[J700         0.107584    0.189043              18.2203       \n",
      "\u001b[J701         0.103483    0.185174              18.2532       \n",
      "\u001b[J702         0.10668     0.204205              18.2835       \n",
      "\u001b[J703         0.110117    0.186913              18.3135       \n",
      "\u001b[J704         0.102353    0.199609              18.3434       \n",
      "\u001b[J705         0.107079    0.203928              18.3734       \n",
      "\u001b[J706         0.11195     0.202407              18.4055       \n",
      "\u001b[J707         0.0929427   0.194537              18.4381       \n",
      "\u001b[J708         0.101748    0.180648              18.4684       \n",
      "\u001b[J709         0.114971    0.194868              18.4984       \n",
      "\u001b[J710         0.104783    0.190021              18.5291       \n",
      "\u001b[J711         0.119076    0.204946              18.5614       \n",
      "\u001b[J712         0.110107    0.18734               18.5915       \n",
      "\u001b[J713         0.0886319   0.208534              18.6218       \n",
      "\u001b[J714         0.110793    0.209448              18.6531       \n",
      "\u001b[J715         0.100784    0.190238              18.6833       \n",
      "\u001b[J716         0.113679    0.17164               18.7152       \n",
      "\u001b[J717         0.0882213   0.241649              18.7458       \n",
      "\u001b[J718         0.110983    0.224273              18.776        \n",
      "\u001b[J719         0.106331    0.191099              18.8063       \n",
      "\u001b[J720         0.103305    0.178625              18.8365       \n",
      "\u001b[J721         0.100186    0.192163              18.8697       \n",
      "\u001b[J722         0.109744    0.225552              18.9001       \n",
      "\u001b[J723         0.104814    0.183068              18.9304       \n",
      "\u001b[J724         0.100517    0.21584               18.961        \n",
      "\u001b[J725         0.112303    0.194999              18.9914       \n",
      "\u001b[J726         0.113976    0.19854               19.0239       \n",
      "\u001b[J727         0.0952671   0.209144              19.0546       \n",
      "\u001b[J728         0.102742    0.183181              19.0863       \n",
      "\u001b[J729         0.10704     0.185262              19.1166       \n",
      "\u001b[J730         0.106209    0.210319              19.1481       \n",
      "\u001b[J731         0.0996663   0.195302              19.1808       \n",
      "\u001b[J732         0.0943933   0.20429               19.2113       \n",
      "\u001b[J733         0.107579    0.193063              19.2421       \n",
      "\u001b[J734         0.0965645   0.18404               19.2727       \n",
      "\u001b[J735         0.110412    0.180701              19.3044       \n",
      "\u001b[J736         0.0948139   0.236155              19.3373       \n",
      "\u001b[J737         0.103111    0.186892              19.368        \n",
      "\u001b[J738         0.119913    0.206026              19.3988       \n",
      "\u001b[J739         0.0886027   0.182718              19.4297       \n",
      "\u001b[J740         0.104743    0.177742              19.4609       \n",
      "\u001b[J741         0.0907515   0.22061               19.494        \n",
      "\u001b[J742         0.124828    0.225762              19.5257       \n",
      "\u001b[J743         0.101004    0.176643              19.5566       \n",
      "\u001b[J744         0.0969532   0.184244              19.5875       \n",
      "\u001b[J745         0.105594    0.231608              19.6184       \n",
      "\u001b[J746         0.0978567   0.187099              19.6513       \n",
      "\u001b[J747         0.106479    0.222698              19.6822       \n",
      "\u001b[J748         0.102698    0.184017              19.7138       \n",
      "\u001b[J749         0.109722    0.179547              19.7451       \n",
      "\u001b[J750         0.0925731   0.256189              19.7758       \n",
      "\u001b[J751         0.0953924   0.186287              19.8087       \n",
      "\u001b[J752         0.107316    0.19787               19.8395       \n",
      "\u001b[J753         0.130119    0.219289              19.8704       \n",
      "\u001b[J754         0.0769496   0.169698              19.9016       \n",
      "\u001b[J755         0.096706    0.198163              19.933        \n",
      "\u001b[J756         0.0885993   0.206495              19.9666       \n",
      "\u001b[J757         0.114111    0.209305              19.9977       \n",
      "\u001b[J758         0.0879286   0.175948              20.029        \n",
      "\u001b[J759         0.100242    0.182548              20.0601       \n",
      "\u001b[J760         0.101393    0.221163              20.0916       \n",
      "\u001b[J761         0.0922151   0.19811               20.1249       \n",
      "\u001b[J762         0.101042    0.201859              20.1566       \n",
      "\u001b[J763         0.0941979   0.193091              20.1885       \n",
      "\u001b[J764         0.0970522   0.201933              20.2199       \n",
      "\u001b[J765         0.10004     0.201586              20.2513       \n",
      "\u001b[J766         0.0875916   0.187299              20.2845       \n",
      "\u001b[J767         0.11639     0.195626              20.3158       \n",
      "\u001b[J768         0.097209    0.229858              20.3473       \n",
      "\u001b[J769         0.13139     0.164066              20.3793       \n",
      "\u001b[J770         0.0947413   0.264023              20.4118       \n",
      "\u001b[J771         0.0905736   0.199483              20.4456       \n",
      "\u001b[J772         0.119708    0.223411              20.477        \n",
      "\u001b[J773         0.091968    0.171035              20.509        \n",
      "\u001b[J774         0.0987996   0.189239              20.5407       \n",
      "\u001b[J775         0.0950741   0.191518              20.5725       \n",
      "\u001b[J776         0.0953091   0.199546              20.6072       \n",
      "\u001b[J777         0.0941392   0.171046              20.6391       \n",
      "\u001b[J778         0.0957452   0.213738              20.6707       \n",
      "\u001b[J779         0.0925266   0.200812              20.7024       \n",
      "\u001b[J780         0.097301    0.186183              20.734        \n",
      "\u001b[J781         0.0856893   0.234237              20.7677       \n",
      "\u001b[J782         0.0943472   0.185988              20.7993       \n",
      "\u001b[J783         0.100944    0.173323              20.8317       \n",
      "\u001b[J784         0.0949088   0.231473              20.8641       \n",
      "\u001b[J785         0.0941744   0.187281              20.8959       \n",
      "\u001b[J786         0.106048    0.192751              20.9298       \n",
      "\u001b[J787         0.0691334   0.202241              20.9619       \n",
      "\u001b[J788         0.0977145   0.193161              20.9938       \n",
      "\u001b[J789         0.096535    0.18858               21.0256       \n",
      "\u001b[J790         0.0922091   0.211321              21.0577       \n",
      "\u001b[J791         0.101915    0.20508               21.092        \n",
      "\u001b[J792         0.084287    0.209581              21.1241       \n",
      "\u001b[J793         0.0914603   0.200209              21.1562       \n",
      "\u001b[J794         0.117748    0.171456              21.1881       \n",
      "\u001b[J795         0.0657459   0.224734              21.2204       \n",
      "\u001b[J796         0.0931626   0.206618              21.2544       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[J797         0.0831162   0.180506              21.2869       \n",
      "\u001b[J798         0.0897896   0.197191              21.3194       \n",
      "\u001b[J799         0.0939939   0.214381              21.3513       \n",
      "\u001b[J800         0.0930489   0.172449              21.3837       \n",
      "\u001b[J801         0.08695     0.215295              21.4176       \n",
      "\u001b[J802         0.099279    0.198993              21.4503       \n",
      "\u001b[J803         0.0974051   0.214855              21.4824       \n",
      "\u001b[J804         0.0879928   0.172645              21.5154       \n",
      "\u001b[J805         0.0926737   0.171685              21.5487       \n",
      "\u001b[J806         0.0871229   0.219877              21.5833       \n",
      "\u001b[J807         0.0932814   0.196663              21.6154       \n",
      "\u001b[J808         0.0867467   0.177127              21.6474       \n",
      "\u001b[J809         0.0891876   0.214636              21.6798       \n",
      "\u001b[J810         0.0932938   0.194838              21.7119       \n",
      "\u001b[J811         0.0940269   0.184755              21.7471       \n",
      "\u001b[J812         0.0979784   0.240476              21.7797       \n",
      "\u001b[J813         0.0985022   0.193721              21.8124       \n",
      "\u001b[J814         0.094164    0.198069              21.8448       \n",
      "\u001b[J815         0.0874952   0.183988              21.8773       \n",
      "\u001b[J816         0.0810433   0.201118              21.9118       \n",
      "\u001b[J817         0.0927292   0.186029              21.9444       \n",
      "\u001b[J818         0.0883643   0.201402              21.9776       \n",
      "\u001b[J819         0.0892148   0.210749              22.0104       \n",
      "\u001b[J820         0.0892107   0.197023              22.0428       \n",
      "\u001b[J821         0.0854112   0.187091              22.0773       \n",
      "\u001b[J822         0.084434    0.19604               22.1098       \n",
      "\u001b[J823         0.0907694   0.194054              22.1423       \n",
      "\u001b[J824         0.0814034   0.212224              22.1756       \n",
      "\u001b[J825         0.0940172   0.202509              22.2095       \n",
      "\u001b[J826         0.0863443   0.197476              22.2443       \n",
      "\u001b[J827         0.0906575   0.201446              22.2767       \n",
      "\u001b[J828         0.0958354   0.258518              22.3094       \n",
      "\u001b[J829         0.082824    0.187692              22.342        \n",
      "\u001b[J830         0.10389     0.169233              22.3748       \n",
      "\u001b[J831         0.0935693   0.226568              22.4098       \n",
      "\u001b[J832         0.0889775   0.191347              22.4435       \n",
      "\u001b[J833         0.093603    0.225553              22.4764       \n",
      "\u001b[J834         0.0929152   0.22117               22.509        \n",
      "\u001b[J835         0.0970893   0.208223              22.5417       \n",
      "\u001b[J836         0.0872383   0.214961              22.5765       \n",
      "\u001b[J837         0.0981351   0.196595              22.6093       \n",
      "\u001b[J838         0.0802372   0.18442               22.6427       \n",
      "\u001b[J839         0.0794919   0.209482              22.6756       \n",
      "\u001b[J840         0.0913553   0.196897              22.7087       \n",
      "\u001b[J841         0.077728    0.21486               22.7436       \n",
      "\u001b[J842         0.0850932   0.183523              22.7765       \n",
      "\u001b[J843         0.0991857   0.221352              22.8094       \n",
      "\u001b[J844         0.083172    0.173881              22.8424       \n",
      "\u001b[J845         0.0914949   0.193441              22.8763       \n",
      "\u001b[J846         0.0780586   0.266443              22.9114       \n",
      "\u001b[J847         0.0942492   0.177768              22.9445       \n",
      "\u001b[J848         0.0996623   0.183978              22.9781       \n",
      "\u001b[J849         0.0815534   0.23194               23.0114       \n",
      "\u001b[J850         0.0885686   0.189199              23.0444       \n",
      "\u001b[J851         0.078705    0.208598              23.0795       \n",
      "\u001b[J852         0.0857766   0.195532              23.1136       \n",
      "\u001b[J853         0.0852369   0.222803              23.1468       \n",
      "\u001b[J854         0.0900214   0.20653               23.1804       \n",
      "\u001b[J855         0.081758    0.187639              23.2137       \n",
      "\u001b[J856         0.0893583   0.214257              23.2491       \n",
      "\u001b[J857         0.0733955   0.17933               23.2822       \n",
      "\u001b[J858         0.0886934   0.176416              23.3163       \n",
      "\u001b[J859         0.0853083   0.227141              23.3499       \n",
      "\u001b[J860         0.0826074   0.221626              23.3832       \n",
      "\u001b[J861         0.0823168   0.171886              23.4185       \n",
      "\u001b[J862         0.0946331   0.212646              23.4521       \n",
      "\u001b[J863         0.0858652   0.204464              23.4855       \n",
      "\u001b[J864         0.0843462   0.183533              23.519        \n",
      "\u001b[J865         0.0902248   0.195485              23.5533       \n",
      "\u001b[J866         0.0774355   0.209954              23.5884       \n",
      "\u001b[J867         0.0842158   0.206336              23.6217       \n",
      "\u001b[J868         0.0845523   0.198302              23.655        \n",
      "\u001b[J869         0.0983771   0.175779              23.6883       \n",
      "\u001b[J870         0.0664214   0.214309              23.722        \n",
      "\u001b[J871         0.0801788   0.1905                23.7586       \n",
      "\u001b[J872         0.0864302   0.195768              23.7924       \n",
      "\u001b[J873         0.0791743   0.234468              23.826        \n",
      "\u001b[J874         0.0776961   0.18286               23.8598       \n",
      "\u001b[J875         0.0803955   0.181312              23.8936       \n",
      "\u001b[J876         0.0779227   0.226773              23.9292       \n",
      "\u001b[J877         0.0876517   0.183481              23.9628       \n",
      "\u001b[J878         0.0805347   0.185071              23.9972       \n",
      "\u001b[J879         0.0861846   0.221094              24.0311       \n",
      "\u001b[J880         0.0815642   0.196505              24.0648       \n",
      "\u001b[J881         0.085683    0.199192              24.1004       \n",
      "\u001b[J882         0.0692364   0.21941               24.1343       \n",
      "\u001b[J883         0.0916054   0.210421              24.1684       \n",
      "\u001b[J884         0.0788634   0.178273              24.2068       \n",
      "\u001b[J885         0.083078    0.190658              24.2788       \n",
      "\u001b[J886         0.0791684   0.236169              24.336        \n",
      "\u001b[J887         0.0777797   0.195072              24.3825       \n",
      "\u001b[J888         0.0774051   0.17576               24.4235       \n",
      "\u001b[J889         0.0950143   0.260553              24.4624       \n",
      "\u001b[J890         0.0835777   0.187241              24.5012       \n",
      "\u001b[J891         0.0801146   0.214111              24.5423       \n",
      "\u001b[J892         0.08291     0.202278              24.5814       \n",
      "\u001b[J893         0.0809466   0.187166              24.6213       \n",
      "\u001b[J894         0.0823058   0.204923              24.6606       \n",
      "\u001b[J895         0.0773362   0.192265              24.6995       \n",
      "\u001b[J896         0.0726104   0.216616              24.7411       \n",
      "\u001b[J897         0.0855987   0.201282              24.78         \n",
      "\u001b[J898         0.0816541   0.216937              24.8192       \n",
      "\u001b[J899         0.0806085   0.209404              24.8595       \n",
      "\u001b[J900         0.0899861   0.177958              24.8988       \n",
      "\u001b[J901         0.0763648   0.235049              24.9401       \n",
      "\u001b[J902         0.0853236   0.208647              24.9793       \n",
      "\u001b[J903         0.0768471   0.195493              25.0184       \n",
      "\u001b[J904         0.0824646   0.191629              25.0577       \n",
      "\u001b[J905         0.0795838   0.199347              25.0979       \n",
      "\u001b[J906         0.0810562   0.226396              25.1385       \n",
      "\u001b[J907         0.0734007   0.241234              25.1732       \n",
      "\u001b[J908         0.110701    0.160862              25.2076       \n",
      "\u001b[J909         0.120839    0.202732              25.2423       \n",
      "\u001b[J910         0.0981101   0.301459              25.277        \n",
      "\u001b[J911         0.0712948   0.172938              25.3141       \n",
      "\u001b[J912         0.106514    0.255033              25.3494       \n",
      "\u001b[J913         0.103955    0.200484              25.3845       \n",
      "\u001b[J914         0.0866289   0.176501              25.422        \n",
      "\u001b[J915         0.0950428   0.258585              25.4565       \n",
      "\u001b[J916         0.0746325   0.19969               25.4931       \n",
      "\u001b[J917         0.116219    0.174553              25.5279       \n",
      "\u001b[J918         0.0540026   0.278955              25.5638       \n",
      "\u001b[J919         0.0825818   0.195321              25.5985       \n",
      "\u001b[J920         0.0912715   0.182566              25.6334       \n",
      "\u001b[J921         0.0897263   0.239461              25.6702       \n",
      "\u001b[J922         0.0803859   0.178552              25.7049       \n",
      "\u001b[J923         0.0912709   0.217215              25.7397       \n",
      "\u001b[J924         0.0927501   0.231762              25.7757       \n",
      "\u001b[J925         0.070206    0.178675              25.8106       \n",
      "\u001b[J926         0.0861534   0.211174              25.8475       \n",
      "\u001b[J927         0.0617062   0.225734              25.8824       \n",
      "\u001b[J928         0.0791319   0.184608              25.9171       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[J929         0.0798256   0.19926               25.9521       \n",
      "\u001b[J930         0.0822546   0.232137              25.988        \n",
      "\u001b[J931         0.0934053   0.163598              26.0255       \n",
      "\u001b[J932         0.0609861   0.231367              26.0606       \n",
      "\u001b[J933         0.0834945   0.234232              26.0961       \n",
      "\u001b[J934         0.0806282   0.186877              26.1309       \n",
      "\u001b[J935         0.0890312   0.171875              26.1663       \n",
      "\u001b[J936         0.0735871   0.335617              26.2053       \n",
      "\u001b[J937         0.0913504   0.193034              26.2404       \n",
      "\u001b[J938         0.087497    0.176633              26.2757       \n",
      "\u001b[J939         0.0747618   0.224108              26.3111       \n",
      "\u001b[J940         0.0851489   0.237145              26.3462       \n",
      "\u001b[J941         0.0717165   0.176336              26.3836       \n",
      "\u001b[J942         0.0858464   0.227522              26.4201       \n",
      "\u001b[J943         0.0743894   0.211396              26.4551       \n",
      "\u001b[J944         0.0772731   0.201056              26.4904       \n",
      "\u001b[J945         0.0762801   0.185269              26.5257       \n",
      "\u001b[J946         0.102228    0.201911              26.5627       \n",
      "\u001b[J947         0.0556007   0.181365              26.5979       \n",
      "\u001b[J948         0.0756703   0.245595              26.634        \n",
      "\u001b[J949         0.0840056   0.219411              26.6696       \n",
      "\u001b[J950         0.069031    0.168                 26.7054       \n",
      "\u001b[J951         0.0795544   0.207795              26.7425       \n",
      "\u001b[J952         0.0836779   0.227876              26.7862       \n",
      "\u001b[J953         0.0771888   0.197702              26.8626       \n",
      "\u001b[J954         0.0802444   0.186579              26.9192       \n",
      "\u001b[J955         0.0728357   0.201776              26.9668       \n",
      "\u001b[J956         0.0763664   0.219118              27.0107       \n",
      "\u001b[J957         0.0760378   0.236257              27.0517       \n",
      "\u001b[J958         0.07376     0.170118              27.0922       \n",
      "\u001b[J959         0.0811252   0.215857              27.133        \n",
      "\u001b[J960         0.0750631   0.216755              27.1742       \n",
      "\u001b[J961         0.0706131   0.196646              27.2168       \n",
      "\u001b[J962         0.0784948   0.181296              27.2576       \n",
      "\u001b[J963         0.0737622   0.243457              27.2989       \n",
      "\u001b[J964         0.0794464   0.197125              27.3396       \n",
      "\u001b[J965         0.0715122   0.188665              27.3806       \n",
      "\u001b[J966         0.0687125   0.200196              27.4235       \n",
      "\u001b[J967         0.0731878   0.232847              27.4644       \n",
      "\u001b[J968         0.0826735   0.209356              27.5063       \n",
      "\u001b[J969         0.0772359   0.218562              27.5472       \n",
      "\u001b[J970         0.0875192   0.158335              27.5881       \n",
      "\u001b[J971         0.0799278   0.249049              27.6311       \n",
      "\u001b[J972         0.0824628   0.183951              27.6723       \n",
      "\u001b[J973         0.0750128   0.19624               27.7139       \n",
      "\u001b[J974         0.0759967   0.202253              27.753        \n",
      "\u001b[J975         0.0742473   0.23092               27.7938       \n",
      "\u001b[J976         0.0691586   0.209213              27.8731       \n",
      "\u001b[J977         0.075686    0.177499              27.9304       \n",
      "\u001b[J978         0.0791836   0.198664              27.9774       \n",
      "\u001b[J979         0.0785875   0.220786              28.0196       \n",
      "\u001b[J980         0.0706005   0.210369              28.0608       \n",
      "\u001b[J981         0.0658617   0.194795              28.1046       \n",
      "\u001b[J982         0.0697288   0.204277              28.1471       \n",
      "\u001b[J983         0.074029    0.200938              28.1888       \n",
      "\u001b[J984         0.0732323   0.213888              28.2303       \n",
      "\u001b[J985         0.0740807   0.200527              28.2718       \n",
      "\u001b[J986         0.0732548   0.19549               28.3157       \n",
      "\u001b[J987         0.0750781   0.205505              28.3581       \n",
      "\u001b[J988         0.0658452   0.235114              28.3994       \n",
      "\u001b[J989         0.0845127   0.193523              28.4407       \n",
      "\u001b[J990         0.0691798   0.230713              28.4819       \n",
      "\u001b[J991         0.074385    0.198888              28.5257       \n",
      "\u001b[J992         0.0727329   0.214999              28.5689       \n",
      "\u001b[J993         0.0768903   0.208801              28.6109       \n",
      "\u001b[J994         0.0704936   0.186742              28.6523       \n",
      "\u001b[J995         0.0753793   0.22132               28.6942       \n",
      "\u001b[J996         0.0645036   0.209353              28.7375       \n",
      "\u001b[J997         0.0714799   0.202207              28.7743       \n",
      "\u001b[J998         0.0738632   0.199553              28.8118       \n",
      "\u001b[J999         0.0769489   0.188117              28.8482       \n",
      "\u001b[J1000        0.0715639   0.22244               28.8884       \n"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の経過データを読み込み\n",
    "\n",
    "Trainerを設定する際に以下のようにout='resut'と指定していました。\n",
    "これにより、学習の結果（上記に表示されているmain/lossやvalidation/main/loss）がresult/logに保存されています。\n",
    "\n",
    "> trainer = training.Trainer(updater, (epoch, 'epoch'), out='result')\n",
    "\n",
    "学習後には、下記のようにlogというファイルが保存されています。\n",
    "<img src=\"./images/08.jpg\" width=\"400\" />\n",
    "\n",
    "こちらのファイルの中身をエディター（メモ帳）で確認してみると、下記のように格納されていました。\n",
    "<img src=\"./images/09.png\" width=\"400\" />\n",
    "\n",
    "こちらのlogファイルを読み込み、可視化してみましょう。\n",
    "\n",
    "ファイルの読み込みは以下のように簡単に実装できます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('result/log') as f:\n",
    "     logs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらをlogsという変数名で格納しておくと、辞書型として以下のようにデータにアクセスできます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の経過をプロット\n",
    "まず、プロットするためにリストに変換しておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_train, result_test = [], []\n",
    "for log in logs:\n",
    "    result_train.append(log['main/loss'])  # 訓練データ\n",
    "    result_test.append(log['validation/main/loss'])  # 訓練データ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちら、Pythonの書き方に慣れている人は以下のように書くこともできます（こちらがPython流）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_train = [ log['main/loss'] for log in logs ]\n",
    "result_test  = [ log['validation/main/loss'] for log in logs ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらだと、空のリストを定義して（[ ]）、そのリストに要素を一つずつ追加して（append）、といった処理を書かなくても良いため、シンプルに書けます。\n",
    "ただし、最初からこちらの書き方だと難しいため、慣れるまでは前者の書き方で、慣れてきたら後者の書き方にしていきましょう。\n",
    "\n",
    "プロットはこれまで同様にmatplotlibを使用しますが、今回は凡例（legend）を追加しておきましょう。\n",
    "plotのオプションにlabelを追加し、plt.legend()を追加することで、凡例が追加されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2ef163a710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(result_train, label='train')  # 訓練データ\n",
    "plt.plot(result_test,  label='test')  # 検証データ\n",
    "plt.legend()  # 凡例表示\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらを確認すると、訓練データの損失関数の値が下がりながら、検証データの損失関数の値も下がっているため、**オーバーフィッティング（過学習）**も起こっておらず、理想的な学習ができていることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの保存と読み込み\n",
    "\n",
    "このように学習したモデルはファイルで保存できます。保存形式は、HDF5形式とNumPy NPZ形式が選べます。今回は追加パッケージのインストールをせずに使えるNPZ形式で保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainer.serializers.save_npz('my_model.model', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、保存したファイルは以下で読み込むことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable W([[-0.16903178, -0.02078728,  0.36140823,  0.34731042,\n",
       "              1.3474458 ,  1.8727336 ,  0.6391152 , -1.5993137 ,\n",
       "              1.5605508 ,  0.04663599],\n",
       "            [-0.0674093 ,  0.0667155 , -0.7158598 , -0.02720177,\n",
       "              0.0847559 ,  1.577785  , -0.7897083 ,  0.5943927 ,\n",
       "             -0.5721071 ,  0.03367344],\n",
       "            [-0.1497474 ,  0.5133939 , -0.128895  ,  0.42519653,\n",
       "              0.16347066, -0.23924273, -0.2702976 , -0.03915106,\n",
       "              0.4630595 , -0.11848157],\n",
       "            [-0.0503069 , -0.43422216,  0.01518925, -0.4965829 ,\n",
       "             -0.40958548,  0.02357351, -0.2788981 , -0.07365122,\n",
       "             -0.30202872,  0.00744722],\n",
       "            [-0.30683094, -0.24167736, -0.08384634, -0.05468265,\n",
       "             -0.5956152 ,  0.50848067,  0.36217642, -0.15292278,\n",
       "              0.3223637 , -0.31175882],\n",
       "            [-0.39979425,  0.74631476,  0.20670186,  0.524924  ,\n",
       "             -1.4302652 , -2.3543067 , -0.14199391,  1.1745796 ,\n",
       "             -1.7268754 ,  0.06691565],\n",
       "            [-0.29657874, -0.438183  , -0.10716344, -0.05668554,\n",
       "             -0.07619422, -0.07832916, -0.05329068,  0.18142278,\n",
       "             -0.30255824, -0.20602952],\n",
       "            [ 0.13347657,  0.29336697, -0.17864245,  0.39432898,\n",
       "             -0.40561157, -0.04476742, -0.06012366, -0.4707988 ,\n",
       "             -0.43397585, -0.09900118],\n",
       "            [-0.23164512,  0.21307851, -0.13753039, -0.32366028,\n",
       "              0.04034018,  0.22703294,  0.12299243,  0.33881965,\n",
       "              0.37258664, -0.28675044],\n",
       "            [ 0.5425759 ,  0.05209354,  0.1590857 , -0.28455892,\n",
       "              0.5160405 , -0.16524763, -0.18082637,  0.45270807,\n",
       "             -0.2664373 , -0.16438988]])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの宣言\n",
    "n_units1, n_units2, n_output = 10, 10, 3\n",
    "loaded_mlp = MLP(n_units1, n_units2, n_output)\n",
    "loaded_model = L.Classifier(loaded_mlp)\n",
    "\n",
    "# モデルにパラメータを読み込ませる\n",
    "chainer.serializers.load_npz('my_model.model', loaded_model)\n",
    "loaded_model.predictor.l1.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. ニューラルネットの学習テクニック (15分)\n",
    "* NNの最適化手法(Momentum法(MomentumSGD)/AdaGrad/Adam)\n",
    "* NNの正則化手法(Dropout)\n",
    "* 勾配消失問題とHeの初期化\n",
    "* Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizerの種類\n",
    "既に登場したSGDやAdam含め、Optimizerにはいくつか種類があります。  \n",
    "詳細は[こちら](https://docs.chainer.org/en/stable/reference/optimizers.html)を御覧ください。\n",
    "<img src=\"./images/10.png\" width=\"400\" />\n",
    "\n",
    "各手法の数式と違いに関しては、下記の記事がわかりやすいです。\n",
    "\n",
    "[Optimizer : 深層学習における勾配法について](http://qiita.com/tokkuman/items/1944c00415d129ca0ee9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD（確率的勾配降下法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モーメンタム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.MomentumSGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.AdaGrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNの正則化手法(Dropout)\n",
    "実データを扱う際にかなり高確率で遭遇する現象として、**NNの過学習（オーバーフィッティング）**があります。\n",
    "<img src=\"./images/11.png\" width=\"400\" />\n",
    "\n",
    "こちらの対策として、**ドロップアウト**があるので、ぜひ覚えておきましょう。\n",
    "ドロップアウトとは、多相ネットワークのユニットを確率的に選別して学習する方法です。\n",
    "ドロップアウトは、train時に指定してあげるだけで完了です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP: Multi-layer Perceptron\n",
    "class MLP(chainer.Chain):\n",
    "    \n",
    "    def __init__(self, n_units1, n_units2, n_output):\n",
    "        super().__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, n_units1)\n",
    "            self.l2 = L.Linear(None, n_units2)\n",
    "            self.l3 = L.Linear(None, n_output)\n",
    "            \n",
    "    def __call__(self, x, train=True):\n",
    "        if train:\n",
    "            h1 = F.dropout( F.relu(self.l1(x)), ratio=0.2)\n",
    "            h2 = F.dropout( F.relu(self.l2(h1)), ratio=0.2 )\n",
    "            return self.l3(h2)\n",
    "        else:\n",
    "            h1 = F.relu(self.l1(x))\n",
    "            h2 = F.relu(self.l2(h1))\n",
    "            return self.l3(h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 勾配消失問題とHeの初期化\n",
    "**勾配消失問題**とは、各層で逆伝播計算を繰り返すうちに、勾配がfloatで処理できる範囲を超えてしまい、正しく重みの更新ができなくなってしまう問題です。これが起きる原因は、各層の重みが大きすぎて勾配が無限大に発散したり、逆に各層の重みが小さすぎて勾配が0になってしまうことが原因です。したがって、各層の重みを適切に初期化することがこの問題を解決する秘訣となります。(他にも様々な工夫がありますが、ここでは扱いません。)\n",
    "\n",
    "初期化方法で有名なものに、XavierやHeなどがあります。\n",
    "各手法の数式と違いに関しては、下記の記事がわかりやすいです。\n",
    "https://qiita.com/m-hayashi/items/02065a2e2ec3e2269e0b\n",
    "\n",
    "reluを使用する際には、Heを使うのが良いと言われていたりします（こちらは経験則）。  \n",
    "詳しくは、機械学習プロフェッショナルシリーズ参照。\n",
    "\n",
    "Chainerで通常のLink宣言では、パラメータの値はランダムに決められるため、こちらをHeベースの初期値にする方法を紹介します。\n",
    "\n",
    "まず、initializer として、以下を定義しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = chainer.initializers.HeNormal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらの initializer を link 宣言の際に、initialWのオプションで指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.links.connection.linear.Linear at 0x7f2ed42cf278>"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chainer.links.Linear(100, 100, initialW=initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "性能を向上させる一つの選択肢として、**Batch Normalization**があります。 \n",
    "Batch Noarmalizationは、深くなった隠れ層において入力を正規化する層を挟むことです。\n",
    "**白色化(whitening)**と呼ばれる手法で、入力の平均を0、標準偏差を1、特徴量間の相関を0にすることで、ニューラルネットワークの収束速度が速くなることが知られています。\n",
    "\n",
    "Batch Normalizationに関しては、下記の記事がわかりやすいです\n",
    "https://deepage.net/deep_learning/2016/10/26/batch_normalization.html\n",
    "\n",
    "こちらをChainerで実装することも非常に簡単です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP: Multi-layer Perceptron\n",
    "class MLP(chainer.Chain):\n",
    "    \n",
    "    def __init__(self, n_units1, n_units2, n_output):\n",
    "        super().__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, n_units1)\n",
    "            self.l2 = L.Linear(None, n_units2)\n",
    "            self.l3 = L.Linear(None, n_output)\n",
    "            \n",
    "    def __call__(self, x):\n",
    "            h1 = L.BatchNormalization( F.relu(self.l1(x)) )\n",
    "            h2 = L.BatchNormalization( F.relu(self.l2(h1)) )\n",
    "            return self.l3(h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. 他手法との比較 (20分)\n",
    "* 汎化誤差・交差検証 (復習)\n",
    "* モデル選択\n",
    "* SVMとNNの比較"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 汎化誤差・交差検証 (復習)\n",
    "### 汎化誤差\n",
    "既にlecture6で学んだと思いますが、**汎化誤差**とは(無限量の)未知のデータに対する誤差のことであり、機械学習ではこれを最小化することが目標です。この汎化誤差を計測するには無限量の未知のデータが必要になるので事実上不可能です。そのため、テスト誤差を汎化誤差の近似とみなして評価を行うことが一般的です。\n",
    "\n",
    "実はこの汎化誤差ですが、4つの項に分解することができます。\n",
    "\n",
    "<img src=\"./images/12.jpg\" width=\"400\" />\n",
    "\n",
    "学習手法が出力する予測関数を$f$とする。このとき、あらゆる予測関数における汎化誤差の最小解を$f^*$、モデル${\\mathcal F}$における最小解を$f_{\\mathcal F}^*$、モデル${\\mathcal F}$における経験損失の最小解を$\\hat f_{\\mathcal F}^*$とします。このとき、(1)、(2)、(3)の各端点における汎化誤差の差がそれぞれ近似誤差、推定誤差、最適化誤差になるます。\n",
    "\n",
    "近似誤差はモデル${\\mathcal F}$の表現力を反映します。推定誤差はデータが有限個しかないことに由来します。推定誤差が大きい状態は過学習と呼ばれ、この項を小さくおさえることは機械学習における重大な関心事です。最後に、最適化誤差は学習アルゴリズムが経験損失の最小解に到達できないことで発生する誤差の増分です。\n",
    "\n",
    "参考：http://www.orsj.or.jp/archive2/or60-4/or60_4_191.pdf\n",
    "\n",
    "### 交差検証\n",
    "#### Hold-out validation\n",
    "テストデータは、汎化誤差を推定するためのデータで、モデル選択に使用するデータではありません。テストデータをモデル選択に使用してしまうと、モデルはテストデータの傾向を学習してしまい、汎化誤差(未知のデータに対する誤差)をテストデータで推定することができません。したがって、テストデータとは別に、モデル選択用の検証データ(validation data)を用意する必要があります。\n",
    "\n",
    "<img src=\"./images/13.png\" width=\"400\" />\n",
    "\n",
    "**Hold-out validation**では、上記のようにデータ全体を訓練データ、検証データ、テストデータに分けます。一般的には、訓練：検証：テストの割合は60:20:20程度ですが、この場合訓練データは全体の60%となってしまい、少量の学習データでは足りなくなる恐れがあります。\n",
    "\n",
    "#### 交差検証(Cross validation)\n",
    "\n",
    "Hold-out validationではデータ量が十分ではない場合、**交差検証(Cross validation)**を利用してデータ量を割増できます。以下のように、データをランダムなk個のfoldに分割し、残りのk-1個のfoldを訓練データとしたモデルで検証誤差を計算します。一般的に、kは5~10程度です。\n",
    "\n",
    "<img src=\"./images/14.png\" width=\"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル選択\n",
    "機械学習のモデルはハイパーパラメータを持ち、その多くはモデルの複雑度に影響を与えます。ここで複雑度というのは、そのモデルがどれだけ複雑なデータを表現できるかを意味します。例えば、他の機械学習のモデルの場合、以下の要素が複雑度に影響を与えます。\n",
    "\n",
    "* ペナルティ項の係数\n",
    "* 基底関数の数\n",
    "\n",
    "Neural Networksの場合、層の種類、層の数、層のunit数などが複雑度に影響を与えます。基本的にはパラメータ数が増えれば増えるほど、そのモデルの表現度が上がり、複雑度が増します。この時、必ずしもモデルを複雑にすれば良いというわけではなく、一般的に以下のことがいえます。\n",
    "\n",
    "<img src=\"./images/15.jpg\" width=\"400\" />\n",
    "\n",
    "* データに対してモデルの複雑度が低い(biasが高いとも言う)場合、訓練データ・テストデータ共に予測誤差が高い\n",
    "* データに対してモデルの複雑度が高い(varianceが高いとも言う)場合、訓練データに対しては予測誤差が低くなるが、テストデータに対しては予測誤差が高い。\n",
    "\n",
    "上記のように、モデルの複雑度は低すぎても良くないし、高すぎても良くないので、適切な複雑度のモデルを選ぶ必要があります。モデル選択をする場合は、グリッドサーチとk分割交差検証を組み合わせたり、入れ子式の交差検証(nested cross-validation)などを利用して、最適なモデルを選択します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVMとNNの比較\n",
    "### 従来の機械学習の学習プロセス\n",
    "SVMを含む従来の機械学習モデルは、Deep Learningに比較してモデルの複雑度は低いことが多いです。なぜなら、Deep Learningは有意義にパラメータを増やしていけますが、従来のモデルの多くはパラメータを増やすのが難しいからです。そのため、従来の機械学習モデルは、生のデータをそのまま入力にするのではなく、そのデータをうまく要約するようにデータを加工します。例えば、画像認識の場合、SIFT特徴量、HOG特徴量など、その画像を表現できるような圧縮されたデータで画像を表現します。それを入力として従来の機械学習モデルは学習を行います。\n",
    "\n",
    "### Deep Learning\n",
    "Deep Learningが出現して、従来の機械学習モデルのように、生のデータを加工して入力する必要はなくなりました。例えば、画像認識の場合、縦256 x 横256 x 色3 次元のデータをネットワークの入力に使えます。今まで人が恣意的に特徴量を作っていましたが、Deep Learningではネットワークが自動的に各層で特徴量を学習していき、最終的な目的(e.g. 画像認識)を達成します。\n",
    "\n",
    "しかし、これで全てが解決したわけではなく、依然としてどのようなネットワークを作ればうまく学習できるかという問題が残っています。人間の工夫する部分が、特徴量設計からネットワーク設計に変化したともいえます。\n",
    "\n",
    "|  | 従来の機械学習モデル | Deep Learning |\n",
    "|:-----------|:------------:|:------------:|\n",
    "| モデルの複雑度 | 低 | 高 |\n",
    "| 特徴量設計 | 必要 | 不要 |\n",
    "| ネットワーク設計 | 不要 | 必要 |\n",
    "| データ量 | 少 | 多 |\n",
    "| left | right | center |\n",
    "| aligned | aligned | aligned |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
